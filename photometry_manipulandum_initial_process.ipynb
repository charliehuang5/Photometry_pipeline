{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from itertools import combinations\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "#These lines allow us to import functions from my python func with helper functions\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/Users/charliehuang/Documents/Photometry_pipeline/data_analysis_helperfuncs')\n",
    "import behav_data_analysis as bd\n",
    "import dlc_helper as dh\n",
    "import behav_helper as bh\n",
    "import cube_helper as ch\n",
    "import photom_helper as ph\n",
    "import statistics_helper as sh\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import importlib\n",
    "importlib.reload(bd)\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10ac99",
   "metadata": {},
   "source": [
    "# Important parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe683c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACK_WINDOW = 1000\n",
    "PRE_MOVE_WINDOW = 140 #0.7 seconds\n",
    "FORWARD_WINDOW = 1000\n",
    "\n",
    "\n",
    "p_BACK_WINDOW = int(30*(BACK_WINDOW/200))\n",
    "p_FORWARD_WINDOW = int(30*(FORWARD_WINDOW/200))\n",
    "p_PRE_MOVE_WINDOW = int(30*(PRE_MOVE_WINDOW/200))\n",
    "\n",
    "datapath = '/Users/charliehuang/Documents/python_work/data/Photometry'\n",
    "manip_folder = '/Photometry_Manipulandum'\n",
    "photom_addon = '_2C3T4B'\n",
    "fluor_folder = '/Photometry_Fluorescence'\n",
    "output_path = datapath + '/Outputs'\n",
    "\n",
    "misc_pkl_folder = datapath + '/misc_pickles'\n",
    "oreg_pkl_file = '/outlier_regions_dictionary.pkl'\n",
    "blacklist_files_m = ['/RR20240320_J_2024-04-26.csv']\n",
    "\n",
    "with open(misc_pkl_folder + oreg_pkl_file, 'rb') as f:\n",
    "    loaded_oreg_dic = pickle.load(f) # deserialize using load()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d3810",
   "metadata": {},
   "source": [
    "# Important Classes and Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb91eb",
   "metadata": {},
   "source": [
    "## CLASS - Gen Cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da546d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mouse:\n",
    "    def __init__(self, name, mouse_folder):\n",
    "        self.name = name\n",
    "        self.mouse_folder = mouse_folder\n",
    "        self.day_2_session = {}\n",
    "    def add_session(self, date, day_dic, manip_file, photom_df):\n",
    "        self.day_2_session[date] = {'day_dic': day_dic, 'manip_file': manip_file, 'photom_df': photom_df}\n",
    "    def days(self):\n",
    "        return self.day_2_session.keys()\n",
    "    \n",
    "class Cage:\n",
    "    def __init__(self):\n",
    "        print(\"fresh new cage\")\n",
    "        self.name_2_mouse = {}\n",
    "    def add_mouse(self, mouse=Mouse):\n",
    "        self.name_2_mouse[mouse.name] = mouse\n",
    "    def get_mouse(self, mouse_name):\n",
    "        return self.name_2_mouse[mouse_name]    \n",
    "    def mice(self):\n",
    "        return self.name_2_mouse.keys()\n",
    "    \n",
    "def full_mouse_name(mouse_ID):\n",
    "    if mouse_ID in ['G','H','I','J','K']:\n",
    "        return 'RR20240320_' + mouse_ID\n",
    "    elif mouse_ID == 'F':\n",
    "        return 'RR20231109_'+mouse_ID\n",
    "    else:\n",
    "        return 'RR20231108_'+mouse_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb35930",
   "metadata": {},
   "source": [
    "## CLASS - Sessions Cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce83963",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_folder = '/Pickles/Manip_BigRun_Pickle'\n",
    "\n",
    "class sessions_cage:\n",
    "    \"\"\"_summary_\n",
    "    Cage that contains sessions directly keyed by their session name (mouse-date)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sessions = {} #dictionary containing sessions\n",
    "    def add_sess(self, key, session):\n",
    "        self.sessions[key] = session\n",
    "    def show_sessions(self):\n",
    "        print(self.sessions.keys())\n",
    "    \n",
    "def load_pickle_file(pkl_file):\n",
    "    print(datapath+pkl_folder+pkl_file)\n",
    "    with open(datapath+pkl_folder+pkl_file, 'rb') as f:\n",
    "        loaded_session = pickle.load(f) # deserialize using load()\n",
    "    f.close()\n",
    "    return loaded_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_cage = sessions_cage()\n",
    "blacklist = ['RR20240320_J-2024_04_26'] # blacklisted out because NO TRIALS (pushes)\n",
    "# Loads pickles from folder into the cage\n",
    "for file in os.listdir(datapath+pkl_folder):\n",
    "    if file.startswith('.'):\n",
    "        continue\n",
    "    key = file.split('.')[0]\n",
    "    if key in blacklist:\n",
    "        continue\n",
    "    obj = load_pickle_file('/' + file)\n",
    "    sess_cage.add_sess(key, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_sessions = list(sess_cage.sessions.keys()) #just a useful variable for lots of plotting/parsing\n",
    "ordered_sessions.sort()\n",
    "ordered_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb47cc",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e8680",
   "metadata": {},
   "source": [
    "## Manip Wrapper (handles behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19117af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_framecount(og_frame_count, mtp):\n",
    "    \"\"\"_summary_\n",
    "        corrects breaks in the framecount (like the framecount jumps back down to 0)\n",
    "    \"\"\"\n",
    "    cands = np.where(np.abs(np.diff(og_frame_count[mtp[0]:mtp[1]])) > 1)[0]\n",
    "    if len(cands) == 0:\n",
    "        return og_frame_count\n",
    "    plt.figure()\n",
    "    plt.plot(og_frame_count)   \n",
    "    plt.title('original frame count, mtp: ' + str(mtp)) \n",
    "    for i,cand in enumerate(cands):\n",
    "        adjust = cand + mtp[0]\n",
    "        new_framesubset = og_frame_count[adjust+1:] - og_frame_count[adjust+1] + og_frame_count[adjust] + 1\n",
    "        new_frame_count = np.hstack([og_frame_count[0:adjust+1], new_framesubset])\n",
    "        assert len(og_frame_count) == len(new_frame_count)\n",
    "        og_frame_count = new_frame_count #update og_frame_count\n",
    "        plt.figure()\n",
    "        plt.plot(og_frame_count)   \n",
    "        plt.title('fix: iter' + str(i)) \n",
    "    return og_frame_count\n",
    "\n",
    "def manip_wrapper(file_m, path, single_trial_vis = None):\n",
    "    \"\"\"_summary_\n",
    "        Run via Big Run Part 1 (refer below). Used for loading in raw behavioral data as well as trial typing\n",
    "        and making behavior cubes\n",
    "        \n",
    "        Loads day_dic. NOTE - several day_dic keys are fronted with \"og\". That is because not all trials (waves) \n",
    "        are used due to photometry related bounds and outliers. Thus later we determine the subset of trials\n",
    "        and make the non-og versions (which are the ones actually used)\n",
    "        \n",
    "    Args:\n",
    "        file_m (str): file name\n",
    "        path (str): path to access file\n",
    "        \n",
    "    Returns:\n",
    "        day_dic: dictionary containing most behaviorally relevant data \n",
    "    \"\"\"\n",
    "\n",
    "    # day_dic['combin_df'], day_dic['col_dic'], day_dic['dlc_df'], day_dic['manip_data'], day_dic['metadata']\n",
    "    day_dic = {}\n",
    "\n",
    "    manip_path = path + file_m\n",
    "    manip_colnames = ['x','y','_','_','_','_','frame_count_1','frame_count_2', 'lick', 'reward', 'robot_state'] #, 'cam_frame']\n",
    "    manip_df = pd.read_csv(manip_path, sep='\\t', lineterminator='\\n', header = None)\n",
    "    manip_data = manip_df.to_numpy()[1:, :]\n",
    "    day_dic['manip_data'] = manip_data\n",
    "    combin_df = pd.DataFrame(data=manip_data, columns=manip_colnames)\n",
    "    col_dic = {col:i for col, i in zip(combin_df.columns,np.arange(combin_df.shape[1]))}\n",
    "\n",
    "    # uncomment if want to look at frame count\n",
    "    # plt.plot(combin_df['frame_count_1'])\n",
    "\n",
    "    # frame mapping from manipulandum to camera\n",
    "    # using frame count\n",
    "    ref_dic = {'frame_count_1' : 6}\n",
    "    manip_trans = dh.determine_manip_trans(manip_data[:,ref_dic['frame_count_1']])\n",
    "\n",
    "    # OUTPUT\n",
    "    # other metadata commented out cuz i dont have the dlc data\n",
    "    metadata  = {'manip_trans_points': manip_trans} #, 'cam_trans_points': cam_trans, 'bodyparts': bodyparts} \n",
    "\n",
    "    #identify any frame count jumps\n",
    "    og_frame_count = combin_df['frame_count_1'].to_numpy()\n",
    "    \n",
    "    # FIX THE FRAME COUNT IF JUNCTIONS\n",
    "    new_frame_count = correct_framecount(og_frame_count, manip_trans) #this is just og_frame_count if there are no junctions\n",
    "    new_frame_count = new_frame_count - new_frame_count[manip_trans[0]] #set first frame (mtp 0) to zero\n",
    "    combin_df['frame_count_1'] = new_frame_count\n",
    "\n",
    "    # OG: from dlc_df load_synced_dfs\n",
    "    # return combin_df, col_dic, dlc_df, manip_data, metadata\n",
    "    day_dic['metadata'] = metadata\n",
    "    day_dic['combin_df'] = combin_df\n",
    "    day_dic['col_dic'] = col_dic\n",
    "\n",
    "    print(day_dic['metadata'])\n",
    "    \n",
    "    # for debugging only\n",
    "    # return ch.number_waveforms_modern(day_dic['manip_data'],  day_dic['metadata'], sing_trial=single_trial_vis,\n",
    "                                                                    #   plot=False, trial_front_half=FORWARD_WINDOW, mini_window=True, vel_threshes=[0.035, 0.015])\n",
    "\n",
    "    # the rest is original\n",
    "    # change marker\n",
    "    # return day_dic\n",
    "    \n",
    "    day_dic['og_waves'], day_dic['og_summary'] = ch.number_waveforms_modern(day_dic['manip_data'],  day_dic['metadata'], sing_trial=single_trial_vis,\n",
    "                                                                      plot=False, trial_front_half=FORWARD_WINDOW, mini_window=True, vel_threshes=[0.035, 0.015])\n",
    "\n",
    "    # og_rew_waves = ch.number_waveforms_reward(day_dic['manip_data'])\n",
    "\n",
    "    # change marker-done commented this out 3/27\n",
    "    # rew_trials = [i for i, wav in enumerate(day_dic['waves']) if wav[2].split('_')[0] == 'rewarded']\n",
    "    # unrew_trials = [i for i, wav in enumerate(day_dic['waves']) if wav[2].split('_')[0] == 'unrewarded']\n",
    "    # day_dic['manip_cube_trials'] = {'rewarded': rew_trials, 'unrewarded': unrew_trials}\n",
    "    \n",
    "    day_dic['behav_mat'] = day_dic['combin_df'].to_numpy() \n",
    "    \n",
    "    # change marker-done\n",
    "    day_dic['og_wcube_all'] = bd.gen_manip_cube(day_dic['behav_mat'], day_dic['og_waves'], back_window=BACK_WINDOW, forward_window=FORWARD_WINDOW)\n",
    "    \n",
    "    # fixed\n",
    "    bh.visualize_behav_cube(day_dic['og_wcube_all'], day_dic['combin_df'].columns, BACK_WINDOW, PRE_MOVE_WINDOW, indiv_trials=True, trial_type='all', dlc_flag=False)\n",
    "\n",
    "    # fixed (doesn't use buffer anymore)\n",
    "    \n",
    "    # change marker-needs to be moved to new wrapper\n",
    "    day_dic['og_manip_dist'] = bh.calc_dist_forcube(day_dic['og_wcube_all'], day_dic['col_dic'], 'x','y', BACK_WINDOW, PRE_MOVE_WINDOW)\n",
    "    day_dic['og_endpoints'] = bh.det_push_endpoints(day_dic['og_wcube_all'], day_dic['og_manip_dist'], BACK_WINDOW, plot=False)\n",
    "    return day_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158a4b5",
   "metadata": {},
   "source": [
    "## Photom Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc670df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "Helper functions for loading in the raw photometry data.\n",
    "\"\"\"\n",
    "\n",
    "def find_subfolder_name(mouse_folder, date, datapath=datapath, fluor_folder=fluor_folder):\n",
    "    subfolders = [a for a in os.listdir(datapath+fluor_folder+mouse_folder) if a[0] != '.']\n",
    "    matched_substring = []\n",
    "    for subfold in subfolders:\n",
    "        if subfold.startswith(date):\n",
    "            matched_substring.append(subfold)\n",
    "    assert len(matched_substring) == 1\n",
    "    return '/'+matched_substring[0] + '/Fluorescence.csv'\n",
    "\n",
    "def photom_wrapper(mouse_folder, date, datapath=datapath, fluor_folder=fluor_folder, title=''):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Loads in the correct photometry fluorescence csv. \n",
    "    Takes care of subtracting background ROI (Ch5) from CH1,2,3,4\n",
    "    Ch2 is DCN, Ch3 is thalamus, Ch4 is SNr, and Ch5 is the control background ROI.\n",
    "    \n",
    "    Returns:\n",
    "        sig_df: pandas dataframe containing photometry data \n",
    "    \"\"\"\n",
    "    file_f = find_subfolder_name(mouse_folder, date, datapath=datapath, fluor_folder=fluor_folder)\n",
    "    print(file_f)\n",
    "    print(datapath, fluor_folder, mouse_folder, file_f)\n",
    "    flur_df = pd.read_csv(datapath + fluor_folder + mouse_folder + file_f, header=1)\n",
    "    print('flur_df columns: ', flur_df.columns)\n",
    "\n",
    "    \n",
    "    dat = {}\n",
    "    cntrl_410 = flur_df['CH5-410']\n",
    "    cntrl_470 = flur_df['CH5-470']\n",
    "    dat['CH1-470'] = flur_df['CH1-470']\n",
    "    dat['CH1-410'] = flur_df['CH1-410']\n",
    "    # Preprocessing: subtract control noise\n",
    "    for i, reg_name in zip([2,3,4], ['DCN', 'Thal', 'SNr']):\n",
    "        dat[reg_name + '-470'] = flur_df['CH'+str(i)+'-470']-cntrl_470\n",
    "        dat[reg_name + '-410'] = flur_df['CH'+str(i)+'-410']-cntrl_410\n",
    "        \n",
    "    sig_df = pd.DataFrame.from_dict(dat)\n",
    "    sig_df.plot(figsize=(20,10))\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    return sig_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b7dd0",
   "metadata": {},
   "source": [
    "## Preprocess Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23791f48",
   "metadata": {},
   "source": [
    "### helper functions (low pass, high pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butter_lowpass(photom, thresh, sampling_rate=30):\n",
    "        b,a = butter(4, thresh, btype='low', fs=sampling_rate) #4th order butterpass\n",
    "        filtered_photom = np.apply_along_axis(lambda x: filtfilt(b, a, x), axis=0, arr=photom)\n",
    "        return filtered_photom\n",
    "\n",
    "def apply_butter_highpass(photom, thresh, sampling_rate=30): #not used in current pipeline\n",
    "    b,a = butter(2, thresh, btype='high', fs=sampling_rate)\n",
    "    filtered_photom = np.apply_along_axis(lambda x: filtfilt(b, a, x, padtype='even'), axis=0, arr=photom)\n",
    "    return filtered_photom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2b8f3",
   "metadata": {},
   "source": [
    "### Main Preprocessing Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(photom, refpoint_framecount, combin_df, phot_coldic, parameter_dic, manip_fps=200, photom_fps=30, plotter_title=''):\n",
    "    \"\"\"_summary_\n",
    "        preprocessing pipeline that is called on a per-trial basis by function - photom_cube_generate - below\n",
    "    \"\"\"\n",
    "    \n",
    "    # STEP -1: Determine photometry frame boundaries\n",
    "    phot_bounds = [refpoint_framecount-p_BACK_WINDOW, refpoint_framecount+p_FORWARD_WINDOW-1]\n",
    "    print(phot_bounds)\n",
    "\n",
    "    # STEP 0: Extracting raw signal (raw signal - control background) \n",
    "    raw_sig = photom.loc[phot_bounds[0]:phot_bounds[1]].to_numpy()\n",
    "    raw_sig_means = np.mean(raw_sig, axis=0)\n",
    "    raw_sig_keys = list(photom.keys())\n",
    "    print('temp photom shape', raw_sig.shape)\n",
    "\n",
    "    # STEP 1: Low Pass Filtering - noise correction\n",
    "    if parameter_dic['lowpass_threshold_2'] == None:\n",
    "        lowpass_photom = apply_butter_lowpass(raw_sig, parameter_dic['lowpass_threshold']) #4th order butterworth lowpass\n",
    "        lowpass_photom_means = np.mean(lowpass_photom, axis=0)\n",
    "        lowpass_photom_keys = list(photom.keys())\n",
    "        print('lowpass photom shape', lowpass_photom.shape)\n",
    "    else: # differentially lowpass 470 and 410 signal\n",
    "        raw_470 = raw_sig[:,[0,2,4,6]]\n",
    "        raw_410 = raw_sig[:,[1,3,5,7]]\n",
    "        arr_470 = apply_butter_lowpass(raw_470, parameter_dic['lowpass_threshold']) #4th order butterworth lowpass\n",
    "        arr_410 = apply_butter_lowpass(raw_410, parameter_dic['lowpass_threshold_2']) #4th order butterworth lowpass\n",
    "        lowpass_photom = np.array([arr_470[:,0],arr_410[:,0],arr_470[:,1],arr_410[:,1],arr_470[:,2],arr_410[:,2],arr_470[:,3],arr_410[:,3]]).T\n",
    "        lowpass_photom_keys = list(photom.keys())\n",
    "        \n",
    "        print('lowpass photom shape', lowpass_photom.shape)\n",
    "\n",
    "    # STEP 2: Motion correction, plot per region\n",
    "    deltf_intermed = {} #Just aligned 410's to the 470s (reg_410adj)\n",
    "    CH470_movcor = {} #(470-410)/410 half number of channels\n",
    "    CH470_410_ratio = {}\n",
    "    CH470_410_uratio = {}\n",
    "    regions =  ['CH1','DCN','Thal','SNr']\n",
    "    for i,reg in enumerate(regions):\n",
    "        chan_470 = lowpass_photom[:,phot_coldic[reg+'-470']]\n",
    "        chan_410 = lowpass_photom[:,phot_coldic[reg+'-410']]\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x=chan_410, y=chan_470) #from scipy.stats\n",
    "        chan_410_fitted = intercept + slope * chan_410\n",
    "        # just shows the adjusted chan 410\n",
    "        deltf_intermed[reg+'-470'] = chan_470#-np.mean(chan_470)\n",
    "        deltf_intermed[reg+'-410adj'] = chan_410_fitted#-np.mean(chan_410_fitted)\n",
    "        \n",
    "        # shows the delta f/f\n",
    "        CH470_movcor[reg+'-470_movcorr'] = (chan_470 - chan_410_fitted)/chan_410_fitted\n",
    "        CH470_410_ratio[reg+'-470_410_ratio'] = chan_470/chan_410_fitted\n",
    "        CH470_410_uratio[reg+'-470_410_uratio'] = chan_470/chan_410\n",
    "\n",
    "    # STEP 2.1: Save CH470 (untouched) and CH410 (linearly aligned to ch470)\n",
    "    deltaf_im_df = pd.DataFrame.from_dict(deltf_intermed)\n",
    "    deltaf_im_keys = list(deltaf_im_df.keys())\n",
    "    deltaf_im_np = deltaf_im_df.to_numpy()\n",
    "\n",
    "    # STEP 2.2: Save movement corrected CH470:  (470-410_a)/410_a\n",
    "    CH470_movcor_df = pd.DataFrame.from_dict(CH470_movcor)\n",
    "    CH470_movcor_keys = list(CH470_movcor_df.keys())\n",
    "    CH470_movcor_np = CH470_movcor_df.to_numpy() #richard comment - call it chan_470_move_cor\n",
    "    \n",
    "    # STEP 3: Normalization to premovement period - zscoring\n",
    "    # uses a premovement window for mean and std\n",
    "    p_start = parameter_dic['norm_window'][0]\n",
    "    p_end = parameter_dic['norm_window'][1]\n",
    "\n",
    "    # STEP 3.1: zscores on CH470_movcor\n",
    "    means = np.mean(CH470_movcor_np[p_start:p_end,:], axis=0) # find means -pre movement window to 0 (where 0 is back_window_p up)\n",
    "    stds = np.std(CH470_movcor_np[p_start:p_end,:], axis=0)\n",
    "    zscores = (CH470_movcor_np-means)/stds #now F/F0\n",
    "    zscores_keys = ['CH1_zscore', 'DCN_zscore', 'Thal_zscore', 'SNr_zscore']\n",
    "\n",
    "\n",
    "    # output_dic contains: numpy arrays \n",
    "    output_dic = {'raw_sig':raw_sig, 'lowpass_photom':lowpass_photom, \n",
    "                  'deltaf_im_np': deltaf_im_np, 'CH470_movcor_np':CH470_movcor_np, 'zscores':zscores}\n",
    "    \n",
    "    # output_dic_keys contains: lists (with the names for channels for respective numpy arrays)\n",
    "    output_dic_keys = {'raw_sig':raw_sig_keys, 'lowpass_photom':lowpass_photom_keys, \n",
    "                       'deltaf_im_np': deltaf_im_keys, 'CH470_movcor_np':CH470_movcor_keys, 'zscores':zscores_keys}    \n",
    "    return output_dic, output_dic_keys\n",
    "\n",
    "def within_oreg(refpoint_framecount, oreg_list):\n",
    "    \"\"\"\n",
    "    Just returns if a photom frame is inside an outlier region (oreg)\n",
    "    - used by photom_cube_generate\n",
    "    \"\"\"\n",
    "    for pair in oreg_list:\n",
    "        # if inside pair's range expanded by forward and backward window\n",
    "        if pair[0] - p_FORWARD_WINDOW <= refpoint_framecount <= pair[1] + p_BACK_WINDOW: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def photom_cube_generate(photom_df, day_dic, oreg_list, \n",
    "                         parameter_dic ={'lowpass_threshold':6, 'norm_window':[p_BACK_WINDOW-p_PRE_MOVE_WINDOW, p_BACK_WINDOW]},\n",
    "                         title='', waves_override=None):\n",
    "    \"\"\"\n",
    "    Main wrapper for generating a photom cube from photom_df\n",
    "    \"\"\"\n",
    "    \n",
    "    phot_coldic = {key:i for i,key in enumerate(photom_df.keys())}\n",
    "    print(phot_coldic)\n",
    "    waves = day_dic['og_waves']\n",
    "    if waves_override != None:\n",
    "        waves = waves_override\n",
    "        print(\"Overriding Waves!\")\n",
    "    mats_dic = {}\n",
    "    rand_mats_dic = {}\n",
    "    \n",
    "    # change marker\n",
    "    trials_used = []\n",
    "    outlier_trials = []\n",
    "    output_dic_keys = []\n",
    "    for trial in range(len(waves)):\n",
    "        print('TRIAL: ' + str(trial))\n",
    "        wave = waves[trial]\n",
    "        refpoint_framecount = int(day_dic['combin_df'].loc[wave[0]]['frame_count_1'])\n",
    "        \n",
    "        upper_phot_frame = (photom_df.shape[0])-30*(FORWARD_WINDOW/200)\n",
    "        lower_phot_frame = 30*(BACK_WINDOW/200)\n",
    "        random_refpoint_framecount = np.random.randint(lower_phot_frame,upper_phot_frame)\n",
    "        \n",
    "        \n",
    "        # trials_used excludes trials which are outside of the bounds\n",
    "        # forward frame is out of photom_df\n",
    "        if refpoint_framecount + 30*(FORWARD_WINDOW/200) > photom_df.shape[0]:\n",
    "            print(\"STOPPING trial addition at trial: \" + str(trial))\n",
    "            break\n",
    "        elif refpoint_framecount - 30*(BACK_WINDOW/200) < 0: #back frame is less than 0\n",
    "            print(\"SKIPPING TRIAL: \", trial)\n",
    "            continue  \n",
    "        elif within_oreg(refpoint_framecount, oreg_list):  \n",
    "            print(\"Trial found inside outlier region\")\n",
    "            outlier_trials.append(trial)\n",
    "            continue  # elif trial in outlier_trials:#     continue #old code\n",
    "        else:\n",
    "            trials_used.append(trial)\n",
    "            output_dic, output_dic_keys = preprocess(photom_df, refpoint_framecount, day_dic['combin_df'], phot_coldic, parameter_dic, plotter_title=title+' trial: ' + str(trial)) #used to also output outlier\n",
    "            \n",
    "            rand_output_dic, rand_output_dic_keys = preprocess(photom_df, random_refpoint_framecount, day_dic['combin_df'], phot_coldic, parameter_dic, plotter_title=title+' trial: ' + str(trial)) #used to also output outlier\n",
    "            for key in output_dic.keys():\n",
    "                if key in mats_dic.keys():\n",
    "                    mats_dic[key].append(output_dic[key])\n",
    "                    rand_mats_dic[key].append(rand_output_dic[key])\n",
    "                else:\n",
    "                    mats_dic[key] = [output_dic[key]]\n",
    "                    rand_mats_dic[key] = [rand_output_dic[key]]\n",
    "    cube_dic = {}\n",
    "    rand_cube_dic = {}\n",
    "    for key in mats_dic.keys():\n",
    "        cube_dic[key] = np.dstack(mats_dic[key])    \n",
    "        rand_cube_dic[key] = np.dstack(rand_mats_dic[key])\n",
    "    return cube_dic, trials_used, outlier_trials, output_dic_keys, rand_cube_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda5000f",
   "metadata": {},
   "source": [
    "# Which files have no frame count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb718409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files with no framecount - skipped in big run - refer to the nonshort manip bigrun ipynb notebook for the function used to determine this list\n",
    "\n",
    "noframe_count = ['RR20231108_A_2023-12-06.csv','RR20231108_A_2023-12-12.csv','RR20231108_B_2023-12-12.csv','RR20231108_B_2023-12-06.csv'\n",
    "                 ,'RR20231108_D_2023-12-06.csv','RR20231108_D_2023-12-12.csv','RR20231108_C_2023-12-12.csv','RR20231108_C_2023-12-06.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091fedc",
   "metadata": {},
   "source": [
    "# Initial Processing Starts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea04aff",
   "metadata": {},
   "source": [
    "## Part 1 - Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc42572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "\n",
    "IMPORTANT - if you loaded in the sess_cage directly (unpickling), don't run this cell\n",
    "- this is for generating a gen cage from the bottom up (starting with behavior)\n",
    "- if you want to just do modifications to the photometry pipeline (ie: preprocessing, outliers, etc) start with big run part 2\n",
    "\n",
    "runs through files in manip_folder and loads behavioral data into day_dic for each session.\n",
    "NOTE - loads files into a gen Cage, NOT a sess_cage\n",
    "\n",
    "avoids:\n",
    "- files in blacklist_files_m (blacklisted cuz no push trials)\n",
    "- files in noframe_count\n",
    "\"\"\"\n",
    "\n",
    "files_m = []\n",
    "mouse_folders = []\n",
    "dates = []\n",
    "for m_folder in os.listdir(datapath + manip_folder):\n",
    "    if m_folder.startswith('.'):\n",
    "        continue\n",
    "    files_m.append('/'+m_folder)\n",
    "files_m.sort()\n",
    "\n",
    "cage2 = Cage()\n",
    "for i,manip_file in enumerate(files_m):\n",
    "    print(\"&&&&&&&&&&&&&\")\n",
    "    print(i, manip_file)\n",
    "    if manip_file in blacklist_files_m:\n",
    "        print(\"SKIPPING blacklist file\")\n",
    "        continue\n",
    "    elif manip_file[1:] in noframe_count:\n",
    "        print(\"SKIPPING (no frame count)\")\n",
    "        continue\n",
    "    \n",
    "    mouse_name = manip_file.split('_')[0]+'_'+manip_file.split('_')[1]\n",
    "    mouse_folder =  mouse_name + photom_addon\n",
    "    mouse_folders.append(mouse_folder)\n",
    "\n",
    "    tempstr = manip_file.split('_')[2]\n",
    "    date = tempstr.split('.')[0]\n",
    "    date = date.replace('-','_')\n",
    "    print(\"mouse_name: \", mouse_name)\n",
    "    day_dic = manip_wrapper(manip_file, datapath+manip_folder)\n",
    "    photom_df = photom_wrapper(mouse_folder, date, title=manip_file)\n",
    "    \n",
    "    \n",
    "    if mouse_name in cage2.name_2_mouse:\n",
    "        cage2.name_2_mouse[mouse_name].add_session(date, day_dic, manip_file, photom_df)\n",
    "    else:\n",
    "        new_mouse = Mouse(mouse_name, mouse_folder)\n",
    "        new_mouse.add_session(date, day_dic, manip_file, photom_df)\n",
    "        cage2.add_mouse(new_mouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d9922",
   "metadata": {},
   "source": [
    "## Part 2 - Generate Photom Cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7595a5b",
   "metadata": {},
   "source": [
    "### Declaration of parameter dics for exploring different preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eba6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_windows = [[p_BACK_WINDOW-p_PRE_MOVE_WINDOW, p_BACK_WINDOW], [p_BACK_WINDOW-p_PRE_MOVE_WINDOW-15, p_BACK_WINDOW-15]]\n",
    "labels = ['minus1', 'minus1_alt']\n",
    "print(norm_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowps = [2,4,6,12] #low pass thresholds\n",
    "\n",
    "pdics_list_temp = []\n",
    "for elem in lowps:\n",
    "    for i,nelem in enumerate(norm_windows):\n",
    "        pdics_list_temp.append([elem,i])\n",
    "pdics_list_temp\n",
    "pdic_list = []\n",
    "for pelem in pdics_list_temp:\n",
    "    pdic = {'lowpass_threshold': pelem[0], 'lowpass_threshold_2':None, 'norm_window': norm_windows[pelem[1]], 'name': str(pelem[0]) + '_' +labels[pelem[1]]}\n",
    "    pdic_list.append(pdic)\n",
    "pdic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d68a18",
   "metadata": {},
   "source": [
    "### Note - only run either one of the cells below (not both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea0925",
   "metadata": {},
   "source": [
    "### Big Run Part 2: FOR SESS CAGE (loaded in from pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sessname in ordered_sessions:\n",
    "    session = sess_cage.sessions[sessname]\n",
    "    parts = sessname.split('-')\n",
    "    oreg_list = loaded_oreg_dic['/' + parts[0] + ' ' + parts[1]]\n",
    "    for parameter_dic in pdic_list:\n",
    "        name = parameter_dic['name']\n",
    "        cube_dic_o, trials_used_o, outliers, cube_dic_keys, rand_cube_dic = photom_cube_generate(session['photom_df'], session['day_dic'], oreg_list, parameter_dic=parameter_dic) \n",
    "        sess_cage.sessions[sessname]['cube_dic_lowp_'+name] = cube_dic_o\n",
    "        sess_cage.sessions[sessname]['rand_cube_dic_lowp_'+name] = rand_cube_dic\n",
    "        sess_cage.sessions[sessname]['outlier_trials'] = outliers\n",
    "        sess_cage.sessions[sessname]['photom_trials_used'] = trials_used_o\n",
    "        sess_cage.sessions[sessname]['cube_dic_keys'] = cube_dic_keys\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfddaaa",
   "metadata": {},
   "source": [
    "### Big Run Part 2: for GEN CAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ff1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# older code to work with cage2\n",
    "for mouse_name in cage2.name_2_mouse.keys():\n",
    "    for date in cage2.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        for parameter_dic in pdic_list:\n",
    "            print('SESSION: ', mouse_name, date)\n",
    "            session = cage2.name_2_mouse[mouse_name].day_2_session[date]\n",
    "            oreg_list = loaded_oreg_dic[mouse_name + ' ' + date]        \n",
    "            name = parameter_dic['name']\n",
    "            cube_dic_o, trials_used_o, outliers, cube_dic_keys, rand_cube_dic = photom_cube_generate(session['photom_df'], session['day_dic'], oreg_list, parameter_dic=parameter_dic) \n",
    "            cage2.name_2_mouse[mouse_name].day_2_session[date]['cube_dic_lowp_'+name] = cube_dic_o\n",
    "            cage2.name_2_mouse[mouse_name].day_2_session[date]['rand_cube_dic_lowp_'+name] = rand_cube_dic\n",
    "            cage2.name_2_mouse[mouse_name].day_2_session[date]['outlier_trials'] = outliers\n",
    "            cage2.name_2_mouse[mouse_name].day_2_session[date]['photom_trials_used'] = trials_used_o\n",
    "            cage2.name_2_mouse[mouse_name].day_2_session[date]['cube_dic_keys'] = cube_dic_keys\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18566679",
   "metadata": {},
   "source": [
    "## Part 3 - Misc Cube Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "Just makes some adjustments listed below\n",
    "- og_waves, og_wcube_all, og_endpoints -> waves, wcube_all, endpoints (these are selected from the ogs to NOT include outliers)\n",
    "    - this just allows waves, wcube_all (the behavior cube) to match the trial-dimension of the photom cube\n",
    "- creates wave_dic : a dictionary containing indices for waves of different trial types (indices of the list waves)\n",
    "- new_endpoints : these are endpoints that are determined by just the robot state changing (so when manipulandum itself\n",
    "    recognized it done)\n",
    "    - i never really used these, but these are a solid alternative for push endpoints(because it's robot state based and not threshold based)\n",
    "\"\"\"\n",
    "\n",
    "def det_new_endpoints(day_dic, wave_type='og_waves'):\n",
    "    waves = day_dic[wave_type]\n",
    "    robo = day_dic['behav_mat'][:,10]\n",
    "    new_endpoints = []\n",
    "    for wave in waves:\n",
    "        robo_period = robo[wave[0]:wave[0]+FORWARD_WINDOW]\n",
    "        robo_trans = np.where(np.diff(robo_period) == -1)[0]\n",
    "        if len(robo_trans) > 0:\n",
    "            trans_point = robo_trans[0] + wave[0] + 1\n",
    "            new_endpoints.append(trans_point)\n",
    "        else:\n",
    "            new_endpoints.append(0)\n",
    "    return new_endpoints\n",
    "\n",
    "for mouse_name in cage2.name_2_mouse.keys():\n",
    "    for date in cage2.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        #photom_trials_used now excldues outliers\n",
    "        session = cage2.name_2_mouse[mouse_name].day_2_session[date]\n",
    "        photom_trials_used = session['photom_trials_used']\n",
    "        \n",
    "        og_waves = session['day_dic']['og_waves']\n",
    "        og_trial_inds = np.arange(len(og_waves))\n",
    "        og_wcube_all = session['day_dic']['og_wcube_all']\n",
    "        \n",
    "        outlier_trials = session['outlier_trials']\n",
    "    \n",
    "        #photom_cube already exlcudes outlier trials and includes photom_trials\n",
    "    \n",
    "        #code to adjust og_waves and og_wcube_all\n",
    "        waves = [wav for i, wav in enumerate(og_waves) if i in photom_trials_used]\n",
    "        wcube_all = og_wcube_all[:,:,photom_trials_used]\n",
    "        \n",
    "        print(mouse_name, date)\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['waves'] = waves\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wcube_all'] = wcube_all\n",
    "\n",
    "        rew_waves = [i for i, wav in enumerate(waves) if wav[2].split('_')[0] == 'rewarded']\n",
    "        unrew_waves = [i for i, wav in enumerate(waves) if wav[2].split('_')[0] == 'unrewarded']\n",
    "        suc_waves = [i for i, wav in enumerate(waves) if wav[2].split('_')[1] == 'success']\n",
    "        fail_waves = [i for i, wav in enumerate(waves) if wav[2].split('_')[1] == 'failure']\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wave_dic'] = {\n",
    "            'rewarded': rew_waves, 'unrewarded': unrew_waves, \n",
    "            'success': suc_waves, 'failure': fail_waves}\n",
    "\n",
    "        #code to adjust og_manip_dist and og_endpoints\n",
    "        og_manip_dist = session['day_dic']['og_manip_dist']\n",
    "        og_endpoints = session['day_dic']['og_endpoints']\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['manip_dist'] = og_manip_dist[:,:,photom_trials_used]\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['endpoints'] = [endpt for i, endpt in enumerate(og_endpoints) if i in photom_trials_used]        \n",
    "        \n",
    "        #new endpoints (using robo state)\n",
    "        new_og_endpoints = det_new_endpoints(session['day_dic'], wave_type='og_waves')\n",
    "        new_endpoints = det_new_endpoints(session['day_dic'], wave_type='waves')\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['new_og_endpoints'] = new_og_endpoints\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['new_endpoints'] = new_endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d54268",
   "metadata": {},
   "source": [
    "## Part 4 - Add rew Photom cubes (newer addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78272c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "Just like part 2, but now centering our trials around the reward pulse (not movement initiation)\n",
    "\"\"\"\n",
    "\n",
    "parameter_dic = {'lowpass_threshold': 2,\n",
    "  'lowpass_threshold_2': None,\n",
    "  'norm_window': [129, 150],\n",
    "  'name': '2_minus1'}\n",
    "for mouse_name in cage2.name_2_mouse.keys():\n",
    "    for date in cage2.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        session = cage2.name_2_mouse[mouse_name].day_2_session[date]\n",
    "        day_dic = session['day_dic']\n",
    "        rew_wave_list = ch.determine_rew_wavelist(day_dic, FORWARD_WINDOW)\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['rew_waves'] = rew_wave_list\n",
    "        rew_waves_validate = [day_dic['waves'][i][0] for i in day_dic['wave_dic']['rewarded']]\n",
    "        assert len(rew_waves_validate) == len(rew_wave_list) #just sanity check\n",
    "        rew_wave_list_mod = [[wave] for wave in rew_wave_list]\n",
    "        oreg_list = loaded_oreg_dic[mouse_name + ' ' + date]        \n",
    "        name = parameter_dic['name']\n",
    "        cube_dic_o, trials_used_o, outliers, cube_dic_keys, rand_cube_dic = photom_cube_generate(session['photom_df'], session['day_dic'], oreg_list, \n",
    "                                                                                                 parameter_dic=parameter_dic, waves_override=rew_wave_list_mod) \n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['rew_cube_dic_lowp_'+name] = cube_dic_o\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['rew_outlier_trials'] = outliers\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['rew_photom_trials_used'] = trials_used_o\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print which sessions do not have any reward cube trials - could be useful downstream\n",
    "for mouse_name in cage2.name_2_mouse.keys():\n",
    "    for date in cage2.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        session = cage2.name_2_mouse[mouse_name].day_2_session[date]\n",
    "        if len(session['rew_cube_dic_lowp_'+name].keys()) == 0:\n",
    "            print(mouse_name + ' ' + date)\n",
    "            \n",
    "rew_ses_notrials = ['RR20231108_B-2023_12_07', 'RR20240320_H-2024_04_19',\n",
    "                    'RR20240320_H-2024_04_23', 'RR20240320_H 2024_04_24',\n",
    "                    'RR20240320_H 2024_04_26']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130ff95",
   "metadata": {},
   "source": [
    "## Part 5 - Pickle sessions from Big Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231eef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_sav_folder = '/Pickles'\n",
    "\n",
    "def serialize_sessions_from_cage(folder, cage):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Pickle a gen cage (ie: cage2)\n",
    "    \"\"\"\n",
    "    for mouse_name in cage.name_2_mouse.keys():\n",
    "        for date in cage.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "            print('SESSION: ', mouse_name, date)\n",
    "            session = cage.name_2_mouse[mouse_name].day_2_session[date]\n",
    "            \n",
    "            fname = mouse_name+'-'+date+'.pkl'\n",
    "            print(fname)\n",
    "            # break\n",
    "            with open(folder+fname, 'wb') as f:  # open a text file\n",
    "                pickle.dump(session, f) # serialize the list\n",
    "            f.close()\n",
    "\n",
    "def serialize_sess_cage(folder, cage):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    (Re)pickle sess_cage\n",
    "    \"\"\"\n",
    "    \n",
    "    for sessname in cage.sessions.keys():\n",
    "        session = cage.sessions[sessname]\n",
    "        fname = '/'+sessname+'.pkl'\n",
    "        print(fname)\n",
    "        # break\n",
    "        with open(folder+fname, 'wb') as f:  # open a text file\n",
    "            pickle.dump(session, f) # serialize the list\n",
    "        f.close()\n",
    "        \n",
    "#example code to load in pkl\n",
    "# with open(pickle_folder+example_ss, 'rb') as f:\n",
    "#     loaded_obj = pickle.load(f) # deserialize using load()\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4bd7c",
   "metadata": {},
   "source": [
    "### only run one of the below saving cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ff226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a sessions cage object (use if loaded in sess_cage)\n",
    "pickle_folder = datapath+pkl_sav_folder+'/Manip_BigRun_Pickle'\n",
    "serialize_sess_cage(pickle_folder, sess_cage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a cage object (use if generating from scratch)\n",
    "pickle_folder = datapath+pkl_sav_folder\n",
    "serialize_sessions_from_cage(pickle_folder, cage2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268f83d",
   "metadata": {},
   "source": [
    "## Part 5.5 Compress cage and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_compressed_cage(sess_cage, session_list, mode='wheel'):\n",
    "    comp_sess_cage = sessions_cage()\n",
    "    if mode == 'manip':\n",
    "        cube_dic_name = 'cube_dic_lowp_2_minus1'\n",
    "        rew_cube_dic_name = 'rew_cube_dic_lowp_2_minus1'\n",
    "        rand_cube_dic_name = 'rand_cube_dic_lowp_2_minus1'\n",
    "        keys_keep = ['photom_df', cube_dic_name, rew_cube_dic_name, rand_cube_dic_name, 'outlier_trials', 'photom_trials_used', 'cube_dic_keys', 'manip_file']\n",
    "        day_dic_keys_keep = ['metadata', 'col_dic', 'waves', 'wcube_all', 'wave_dic', 'manip_dist', 'endpoints', 'new_endpoints']\n",
    "    elif mode == 'wheel':\n",
    "        cube_dic_name = 'cube_dic_lowp_2_minus1_alt'\n",
    "        rand_cube_dic_name = 'rand_cube_dic_lowp_2_minus1_alt'\n",
    "        keys_keep = ['photom_df', cube_dic_name, rand_cube_dic_name, 'outlier_trials', 'photom_trials_used', 'cube_dic_keys', 'dlc_file', 'rad_file']\n",
    "        day_dic_keys_keep = ['waves','wcube_all','wave_dic','trial_defs','wheel_trans','stride_stance_dic','hand_peaks_troughs','foot_peaks_troughs']\n",
    "    for sessname in session_list:\n",
    "        if sessname in blacklist:\n",
    "            continue\n",
    "        session = sess_cage.sessions[sessname]\n",
    "        day_dic = session['day_dic']\n",
    "        day_dic_keep = dict((k, day_dic[k]) for k in day_dic_keys_keep)\n",
    "        session_keep = dict((k, session[k]) for k in keys_keep)\n",
    "        session_keep.update({'day_dic':day_dic_keep})\n",
    "        comp_sess_cage.add_sess(sessname, session_keep)\n",
    "    return comp_sess_cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function gen_compressed_cage is located in the \"HELPER - Compress Cage\" section under Important Classes and Helpers\n",
    "# refer there to see which attributes of sessions are kept and not kept\n",
    "\n",
    "compressed_manip_pkl_folder = datapath + pkl_sav_folder + '/Compressed_Manip'\n",
    "compressed_manip_cage = gen_compressed_cage(sess_cage, ordered_sessions, mode='manip')\n",
    "serialize_sess_cage(compressed_manip_pkl_folder, compressed_manip_cage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b9ca6",
   "metadata": {},
   "source": [
    "# Big run Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1627c",
   "metadata": {},
   "source": [
    "## Control: front half back half trial typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82414892",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mouse_name in cage2.name_2_mouse.keys():\n",
    "    for date in cage2.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        session = cage2.name_2_mouse[mouse_name].day_2_session[date]    \n",
    "        waves = session['day_dic']['waves']\n",
    "        front_half = np.arange(int(len(waves)/2))\n",
    "        back_half = np.arange(int(len(waves)/2), len(waves))\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wave_dic']['front_half'] = front_half\n",
    "        cage2.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wave_dic']['back_half'] = back_half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f444710",
   "metadata": {},
   "source": [
    "## Vis Part 1: Plotting EACH Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c00df4",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60543198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cubedics(parameter_dic, cube_dic_type, \n",
    "                  ylim_dic = {'raw_sig': [-1.5,1.5], 'lowpass_photom': [-1.5,1.5], 'deltaf_im_np': [-1,1], 'CH470_movcor_np': [-0.005,0.005], 'CH470_410_ratio_np': [0.998,1.002],\n",
    "                              'zscores': [-4,4], 'zscores_ratio': [-4,4], 'f_f0': [0.998,1.002],\n",
    "                             'CH470_410_uratio_np' : [0.998,1.002], 'zscores_uratio': [-4,4], 'f_f0_u': [0.998,1.002]\n",
    "                             },\n",
    "                  care_about = [True,True,True], heatmap=True,\n",
    "                  save_subfolder='', save_genfolder = output_path + '/Preprocessing', save_label='', no_ylim=False) :\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    -----\n",
    "    Plots and (optionally) saves plots of ch1, dcn, thal, snr - averaged across TRIALS\n",
    "    NOTE - if save_subfolder left as '', no saving occurs\n",
    "    NOTE - currently has two breaks in the for loop (put there during debugging), remove these if want to use this and plot ALL sessions\n",
    "    \n",
    "    Parameters\n",
    "    -----\n",
    "    parameter_dic: dictionary of preprocessing params. should use a parameter_dic created in Big Run Part 2\n",
    "    \n",
    "    cube_dic_type: string selected from ['raw_sig', 'lowpass_photom', 'deltaf_im_np', 'deltaf_np', 'zscores']\n",
    "\n",
    "        # select_from: 'raw_sig', 'lowpass_photom' 'deltaf_im_np' 'CH470_movcor_np' 'zscores's\n",
    "    \n",
    "    care_about = if we want to plot [all_sess, rewarded, unrewarded]\n",
    "\n",
    "    \"\"\"\n",
    "    lowp_thresh_used = parameter_dic['lowpass_threshold']\n",
    "    addon = '_lowp_'+str(lowp_thresh_used)\n",
    "    norm_window = parameter_dic['norm_window'] \n",
    "    if no_ylim:\n",
    "        ylim=None\n",
    "    else:\n",
    "        ylim = ylim_dic[cube_dic_type]\n",
    "    for mouse_name in cage2.name_2_mouse.keys():\n",
    "        for date in cage2.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "            print(mouse_name, date)\n",
    "            session = cage2.name_2_mouse[mouse_name].day_2_session[date]    \n",
    "            cube_dic = session['cube_dic'+addon]\n",
    "            cube_dic_keys = session['cube_dic_keys'][cube_dic_type]\n",
    "            # print(\"Cube dic keys: \")\n",
    "            # print(cube_dic_keys)\n",
    "            col_dic = {key: i for i,key in enumerate(cube_dic_keys)}\n",
    "            \n",
    "            if len(list(cube_dic.keys())) == 0:\n",
    "                print(\"NO WAVES\")\n",
    "                continue\n",
    "            cube = cube_dic[cube_dic_type]     \n",
    "            waves = session['day_dic']['waves']\n",
    "            rewarded_waves = session['day_dic']['wave_dic']['rewarded']\n",
    "            unrewarded_waves = session['day_dic']['wave_dic']['unrewarded']\n",
    "            # print(len(waves), len(rewarded_waves), len(unrewarded_waves))\n",
    "            \n",
    "            #all waves\n",
    "            all_title = mouse_name + '_' + date + ' ' + str(cube.shape[2]) + ' trials'\n",
    "            save_title = mouse_name + '-' + date + '-' + cube_dic_type + '-' + save_label + '.jpg'\n",
    "            save_path = save_genfolder + save_subfolder\n",
    "            save_flag = save_subfolder != ''\n",
    "            \n",
    "            norm_window = parameter_dic['norm_window']\n",
    "            \n",
    "            if care_about[0]:\n",
    "                ph.visualize_cube(cube, col_dic, time_offset = BACK_WINDOW/200, title=all_title, ylim=ylim, norm_window=norm_window, save_flag=save_flag, save_path=save_path, save_title=save_title, heatmap=heatmap)\n",
    "            #rewarded\n",
    "            if len(rewarded_waves) > 0 and care_about[1]:\n",
    "                rew_photom_cube = cube[:,:,rewarded_waves]\n",
    "                rew_title = 'Rewarded: ' + mouse_name + '_' + date + ' ' + str(len(rewarded_waves)) + ' trials '\n",
    "                save_title = save_title[:-5] + '-Rew.jpg'\n",
    "                ph.visualize_cube(rew_photom_cube, col_dic, time_offset = BACK_WINDOW/200, title=rew_title, ylim=ylim, norm_window=norm_window, save_flag=save_flag, save_path=save_path, save_title=save_title, heatmap=heatmap)\n",
    "            #unrewarded\n",
    "            if len(unrewarded_waves) > 0 and care_about[2]:\n",
    "                unrew_photom_cube = cube[:,:,unrewarded_waves]\n",
    "                unrew_title = 'Unrewarded: ' + mouse_name + '_' + date + ' ' + str(len(unrewarded_waves)) + ' trials ' \n",
    "                save_title = save_title[:-5] + '-Unrew.jpg'\n",
    "                ph.visualize_cube(unrew_photom_cube, col_dic, time_offset = BACK_WINDOW/200, title=unrew_title, ylim=ylim, norm_window=norm_window, save_flag=save_flag, save_path=save_path, save_title=save_title, heatmap=heatmap)\n",
    "\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991de964",
   "metadata": {},
   "source": [
    "## VP1 - pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c46d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'raw_sig', 'lowpass_photom' 'deltaf_im_np' 'CH470_movcor_np' 'zscores's\n",
    "cube_dic_type = 'zscores'\n",
    "plot_cubedics(parameter_dic, cube_dic_type, save_subfolder='', save_label='', care_about=[True,False,False], no_ylim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f0815",
   "metadata": {},
   "source": [
    "## Vis Part 2: parsing + avg cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af46fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parsing ordered_sessions (from sess_cage) to get groups_dic\n",
    "groups_dic: a dictionary mapping time_zone ('early','mid','late') to a list of corresponding session names (to use to access sess_cage)\n",
    "\"\"\"\n",
    "\n",
    "ghikj_early_days = ['2024_04_15','2024_04_16','2024_04_17']\n",
    "ghikj_mid_days = ['2024_04_19','2024_04_22']\n",
    "ghik_late_days = ['2024_04_24','2024_04_25','2024_04_26']\n",
    "j_late_days = ['2024_04_23','2024_04_24','2024_04_25']\n",
    "\n",
    "abcd_early_days = ['2023_12_05','2023_12_07','2023_12_08']\n",
    "f_early_days = ['2024_01_15','2024_01_16','2024_01_17']\n",
    "abcd_late_days = ['2023_12_13','2023_12_14','2023_12_15']\n",
    "f_late_days = ['2024_01_24','2024_01_25','2024_01_26']\n",
    "f_mid_days = ['2024_01_19','2024_01_22']\n",
    "\n",
    "early_sessions, late_sessions, mid_sessions = [],[],[]\n",
    "for ses in ordered_sessions:\n",
    "    date = ses.split('-')[1]\n",
    "    mouse_ID = ses.split('-')[0][-1] \n",
    "    \n",
    "    if date in abcd_early_days or date in f_early_days or date in ghikj_early_days:\n",
    "        early_sessions.append(ses)\n",
    "    elif date in abcd_late_days or date in f_late_days:\n",
    "        late_sessions.append(ses)\n",
    "    elif mouse_ID == 'J' and date in j_late_days:\n",
    "        late_sessions.append(ses)\n",
    "    elif mouse_ID != 'J' and date in ghik_late_days:\n",
    "        late_sessions.append(ses)\n",
    "    else:\n",
    "        if mouse_ID in ['G','H','I','K','J'] and date in ghikj_mid_days:\n",
    "            mid_sessions.append(ses)\n",
    "        elif mouse_ID == 'F' and date in f_mid_days:\n",
    "            mid_sessions.append(ses)\n",
    "        elif mouse_ID not in ['G','H','I','K','J','F']:\n",
    "            mid_sessions.append(ses)\n",
    "\n",
    "groups_dic = {'early':early_sessions, 'mid': mid_sessions, 'late': late_sessions}\n",
    "print(groups_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_behav_cube_lis(session_list, parameter_dic, sess_cage, cube_dic_type = 'zscores', trial_type=None):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -----\n",
    "    list of manip_velocity cubes\n",
    "    \"\"\"\n",
    "    cube_list = []\n",
    "    ses_skip = []\n",
    "    cube_dic_keys = None\n",
    "    for sessname in session_list:\n",
    "        # parts = sessname.split('-')\n",
    "        # session = cage2.name_2_mouse['/' + parts[0]].day_2_session[parts[1]] #used before loading in the cube\n",
    "        session = sess_cage.sessions[sessname] #mod: May 30 2024\n",
    "        manip_dist = session['day_dic']['manip_dist']\n",
    "        cube = np.diff(manip_dist, axis=0) #velocity = derivative of manip_dist over time\n",
    "        # manip_vel_mean = np.mean(manip_vel, axis=2) #average over trials\n",
    "        if trial_type != None:\n",
    "            wave_inds = session['day_dic']['wave_dic'][trial_type]\n",
    "            if len(wave_inds) == 0:\n",
    "                ses_skip.append(sessname)\n",
    "                print(sessname)\n",
    "            cube = cube[:,:,wave_inds]\n",
    "        cube_list.append(cube)\n",
    "    return cube_list, cube_dic_keys, ses_skip\n",
    "\n",
    "\n",
    "def gen_cube_list(session_list, parameter_dic, sess_cage, cube_dic_type = 'zscores', trial_type=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----\n",
    "    cube_dic_type: string selected from ['raw_sig', 'lowpass_photom', 'deltaf_im_np', 'deltaf_np', 'zscores']\n",
    "    \"\"\"\n",
    "    cube_list = []\n",
    "    lowp_thresh_used = parameter_dic['lowpass_threshold']\n",
    "    # addon = '_lowp_'+str(lowp_thresh_used)\n",
    "    addon = '_lowp_' + str(parameter_dic['name'])\n",
    "    ses_skip = []\n",
    "    for sessname in session_list:\n",
    "        parts = sessname.split('-')\n",
    "        # session = cage2.name_2_mouse['/' + parts[0]].day_2_session[parts[1]] #used before loading in the cube\n",
    "        session = sess_cage.sessions[sessname] #mod: May 30 2024\n",
    "        cube_dic = session['cube_dic'+addon]\n",
    "        cube = cube_dic[cube_dic_type]\n",
    "        if trial_type != None:\n",
    "            # print(session['day_dic']['wave_dic'])\n",
    "            wave_inds = session['day_dic']['wave_dic'][trial_type]\n",
    "            if len(wave_inds) == 0:\n",
    "                ses_skip.append(sessname)\n",
    "                print(sessname)\n",
    "            cube = cube[:,:,wave_inds]\n",
    "        cube_dic_keys = session['cube_dic_keys'][cube_dic_type]\n",
    "        cube_list.append(cube)\n",
    "    return cube_list, cube_dic_keys, ses_skip\n",
    "\n",
    "def multi_cube_plot(session_list, cube_list, cube_params, cube_dic_type, parameter_dic, title_addon='',\n",
    "                    save_subfolder='', save_genfolder = output_path + '/Preprocessing', save_label='',\n",
    "                   ylim_dic = {'raw_sig': [-1.5,1.5], 'lowpass_photom': [-1.5,1.5], 'deltaf_im_np': [-1,1], 'deltaf_np': [-0.005,0.005], 'zscores': [-4,4]},\n",
    "                   behavior_flag = False):\n",
    "    # master_cubelist_params = ['DCN average zscore','Thal average zscore','SNr average zscore']\n",
    "    col_dic = {elem: i for i, elem in enumerate(cube_params)}    \n",
    "    master_cubelist = []\n",
    "    daycube_list = [np.nanmean(daycube, axis=2) for daycube in cube_list] #list of cubes averaged across trials\n",
    "    mouse_ids, grouped_daycubes = sh.only_group_by_mice(session_list, daycube_list) #makes a list of cubes per mouse (sublists in the list grouped_daycubes)\n",
    "    sesscube_list = [np.dstack(daycube_sublist) for daycube_sublist in grouped_daycubes]\n",
    "    mastercube_list = [np.nanmean(sesscube, axis=2) for sesscube in sesscube_list]\n",
    "    mastercube = np.dstack(mastercube_list)\n",
    "    title=cube_dic_type + '-' + str(parameter_dic) + '-' + title_addon\n",
    "\n",
    "    save_title = '/' + save_label + '-' + cube_dic_type + '.jpg'\n",
    "    save_path = save_genfolder + save_subfolder\n",
    "    save_flag = save_subfolder != ''\n",
    "    norm_window=parameter_dic['norm_window']\n",
    "    if not behavior_flag:\n",
    "        ph.visualize_cube(mastercube, col_dic, BACK_WINDOW/200, title=title, norm_window=norm_window,\n",
    "                      save_flag=save_flag, save_path=save_path, save_title=save_title,\n",
    "                      plot_3D=False, xlabel='Time (s)', ylabel='Z-Score', ylim=ylim_dic[cube_dic_type])\n",
    "    else:\n",
    "        new_norm_win = [(val/30)*200 for val in parameter_dic['norm_window']]\n",
    "        ph.visualize_master_behavcube(mastercube, new_norm_win, 200, 5)\n",
    "        # def visualize_master_behavcube(cube, norm_window, frame_rate, time_offset, title= '', save_flag=False, save_path = '', save_title = '',heatmap=True):\n",
    "    return mastercube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd2efb",
   "metadata": {},
   "source": [
    "## VP2 - SAVING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa8e9a",
   "metadata": {},
   "source": [
    "### Saving - NO trial types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a12bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_dic_type = 'zscores'\n",
    "param_dic_manip = {'lowpass_threshold': 2,\n",
    "  'lowpass_threshold_2': None,\n",
    "  'norm_window': [129, 150],\n",
    "  'name': '2_minus1'}\n",
    "\n",
    "cube_list, cube_dic_keys, ses_skip = gen_cube_list(ordered_sessions, param_dic_manip, sess_cage, cube_dic_type=cube_dic_type)\n",
    "master_cube_all = multi_cube_plot(ordered_sessions,cube_list,cube_dic_keys, cube_dic_type, param_dic_manip,\n",
    "                                  save_genfolder=output_path + '/Manip_Photom_figures', save_subfolder='/allses',save_label='alltrials')\n",
    "\n",
    "b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(ordered_sessions, param_dic_manip, sess_cage, cube_dic_type=cube_dic_type)\n",
    "b_master_cube_all = multi_cube_plot(ordered_sessions,b_cube_list,cube_dic_keys, cube_dic_type, param_dic_manip, save_subfolder='',\n",
    "                                    behavior_flag=True)\n",
    "subfolder = '/allses'\n",
    "# ph.save_cube(master_cube_all,  subfolder, 'alltrials', cube_type = cube_dic_type)\n",
    "# ph.save_cube(b_master_cube_all,  subfolder, 'alltrials_behav', cube_type = cube_dic_type, behav=True)\n",
    "\n",
    "time_z_cubes = []\n",
    "for time_z in groups_dic.keys():\n",
    "    cube_list, cube_dic_keys, cube_mean_vals = gen_cube_list(groups_dic[time_z], param_dic_manip, sess_cage, cube_dic_type=cube_dic_type)\n",
    "    cube = multi_cube_plot(groups_dic[time_z],cube_list,cube_dic_keys, cube_dic_type, param_dic_manip,\n",
    "                           save_genfolder=output_path + '/Manip_Photom_figures', save_subfolder='/'+time_z,save_label='alltrials')\n",
    "    \n",
    "    b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(groups_dic[time_z], param_dic_manip, sess_cage, cube_dic_type=cube_dic_type)\n",
    "    b_master_cube_all = multi_cube_plot(groups_dic[time_z],b_cube_list,cube_dic_keys, cube_dic_type, param_dic_manip,save_subfolder='',\n",
    "                                        behavior_flag=True)\n",
    "    subfolder = '/' + time_z\n",
    "    # ph.save_cube(cube,  subfolder, 'alltrials', cube_type = cube_dic_type)\n",
    "    # ph.save_cube(b_master_cube_all,  subfolder, 'alltrials_behav', cube_type = cube_dic_type, behav=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb1fc1",
   "metadata": {},
   "source": [
    "### Saving - YES trial types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_dic_type = 'zscores'\n",
    "param_dic_manip = {'lowpass_threshold': 2,\n",
    "  'lowpass_threshold_2': None,\n",
    "  'norm_window': [129, 150],\n",
    "  'name': '2_minus1'}\n",
    "for trial_type in ['rewarded','unrewarded']:\n",
    "    cube_list, cube_dic_keys, ses_skip = gen_cube_list(ordered_sessions, param_dic_manip, sess_cage, cube_dic_type=cube_dic_type,\n",
    "                                                       trial_type=trial_type)\n",
    "    master_cube_all = multi_cube_plot(ordered_sessions,cube_list,cube_dic_keys, cube_dic_type, param_dic_manip,\n",
    "                                      title_addon='alltime_' + trial_type,\n",
    "                                      save_genfolder=output_path + '/Manip_Photom_figures', save_subfolder='/allses',save_label=trial_type)\n",
    "\n",
    "    b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(ordered_sessions, param_dic_manip, sess_cage, cube_dic_type=cube_dic_type,\n",
    "                                                        trial_type=trial_type)\n",
    "    b_master_cube_all = multi_cube_plot(ordered_sessions,b_cube_list,cube_dic_keys, cube_dic_type, param_dic_manip, save_subfolder='',\n",
    "                                        behavior_flag=True, title_addon='alltime_' + trial_type)\n",
    "    subfolder = '/allses'\n",
    "    # ph.save_cube(master_cube_all,  subfolder, trial_type, cube_type = cube_dic_type)\n",
    "    # ph.save_cube(b_master_cube_all,  subfolder, trial_type + '_behav', cube_type = cube_dic_type, behav=True)\n",
    "\n",
    "    time_z_cubes = []\n",
    "    for time_z in groups_dic.keys():\n",
    "        cube_list, cube_dic_keys, cube_mean_vals = gen_cube_list(groups_dic[time_z], param_dic_manip, sess_cage, cube_dic_type=cube_dic_type,\n",
    "                                                                 trial_type=trial_type)\n",
    "        cube = multi_cube_plot(groups_dic[time_z],cube_list,cube_dic_keys, cube_dic_type, param_dic_manip,\n",
    "                               title_addon=time_z + '_' + trial_type,\n",
    "                               save_genfolder=output_path + '/Manip_Photom_figures', save_subfolder='/'+time_z,save_label=trial_type)\n",
    "        \n",
    "        b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(groups_dic[time_z], param_dic_manip, sess_cage, cube_dic_type=cube_dic_type,\n",
    "                                                                    trial_type=trial_type)\n",
    "        b_master_cube_all = multi_cube_plot(groups_dic[time_z],b_cube_list,cube_dic_keys, cube_dic_type, param_dic_manip,save_subfolder='',\n",
    "                                            behavior_flag=True, title_addon=time_z + '_' + trial_type)\n",
    "        subfolder = '/' + time_z\n",
    "        # ph.save_cube(cube,  subfolder, trial_type, cube_type = cube_dic_type)\n",
    "        # ph.save_cube(b_master_cube_all,  subfolder, trial_type + '_behav', cube_type = cube_dic_type, behav=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
