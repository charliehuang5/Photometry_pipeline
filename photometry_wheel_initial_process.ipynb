{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy\n",
    "import cv2\n",
    "import random\n",
    "from itertools import combinations\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "#These lines allow us to import functions from my python func with helper functions\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/Users/charliehuang/Documents/Photometry_pipeline/data_analysis_helperfuncs')\n",
    "import behav_data_analysis as bd\n",
    "import dlc_helper as dh\n",
    "import wheel_helper as wh\n",
    "import photom_helper as ph\n",
    "import statistics_helper as sh\n",
    "import stride_stance_helper as stsh\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import importlib\n",
    "importlib.reload(bd)\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used yet\n",
    "WARP_LENGTH = 1000\n",
    "\n",
    "BACK_WINDOW = 500\n",
    "PRE_MOVE_WINDOW = 70 # 0.7 seconds\n",
    "FORWARD_WINDOW = 500\n",
    "\n",
    "p_BACK_WINDOW = int(30*(BACK_WINDOW/100))\n",
    "p_FORWARD_WINDOW = int(30*(FORWARD_WINDOW/100))\n",
    "p_PRE_MOVE_WINDOW = int(30*(PRE_MOVE_WINDOW/100))\n",
    "\n",
    "datapath = '/Users/charliehuang/Documents/python_work/data/Photometry'\n",
    "manip_folder = '/Photometry_Manipulandum'\n",
    "\n",
    "photom_addon = '_2C3T4B'\n",
    "fluor_folder = '/Photometry_Fluorescence'\n",
    "\n",
    "arduino_folder = '/Photometry_Wheel'\n",
    "radians_folder = '/radians'\n",
    "dlc_folder = '/DLC'\n",
    "rename_dic = {'A':'G', 'B':'H', 'C':'I', 'D':'J', 'E':'K'}\n",
    "output_path = datapath + '/Outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Classes and Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class - Gen Cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mouse:\n",
    "    def __init__(self, name, mouse_folder):\n",
    "        self.name = name\n",
    "        self.mouse_folder = mouse_folder\n",
    "        self.day_2_session = {}\n",
    "    def add_session(self, date, day_dic, rad_file, dlc_file, photom_df):\n",
    "        self.day_2_session[date] = {'day_dic': day_dic, 'rad_file': rad_file, 'dlc_file': dlc_file, 'photom_df': photom_df}\n",
    "    \n",
    "class Cage:\n",
    "    def __init__(self):\n",
    "        print(\"fresh new cage\")\n",
    "        # self.date_2_mouse = {}\n",
    "        self.name_2_mouse = {}\n",
    "    def add_mouse(self, mouse=Mouse):\n",
    "        self.name_2_mouse[mouse.name] = mouse\n",
    "    def get_mouse(self, mouse_name):\n",
    "        return self.name_2_mouse[mouse_name]    \n",
    "    \n",
    "\n",
    "def full_mouse_name(mouse_ID):\n",
    "    if mouse_ID in ['G','H','I','J','K']:\n",
    "        return 'RR20240320_' + mouse_ID\n",
    "    elif mouse_ID == 'F':\n",
    "        return 'RR20231109_'+mouse_ID\n",
    "    else:\n",
    "        return 'RR20231108_'+mouse_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS - Sessions Cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_folder = '/Pickles/Wheel_BigRun_Pickle'\n",
    "\n",
    "class sessions_cage:\n",
    "    def __init__(self):\n",
    "        self.sessions = {}\n",
    "    def add_sess(self, key, session):\n",
    "        self.sessions[key] = session\n",
    "    def show_sessions(self):\n",
    "        print(self.sessions.keys())\n",
    "    \n",
    "def load_pickle_file(pkl_file):\n",
    "    print(datapath+pkl_folder+pkl_file)\n",
    "    with open(datapath+pkl_folder+pkl_file, 'rb') as f:\n",
    "        loaded_session = pickle.load(f) # deserialize using load()\n",
    "    f.close()\n",
    "    return loaded_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_cage = sessions_cage()\n",
    "for file in os.listdir(datapath + pkl_folder):\n",
    "    if file.startswith('.'):\n",
    "        continue\n",
    "    key = file.split('.')[0]\n",
    "    obj = load_pickle_file('/' + file)\n",
    "    sess_cage.add_sess(key, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_sessions = list(sess_cage.sessions.keys())\n",
    "ordered_sessions.sort()\n",
    "ordered_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radians Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_frame_count(radians, wheel_trans, photom_df_length):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Uses wheel transition points (wheel_trans) to interpolate the frame count for the radians\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_rate = (wheel_trans[1]-wheel_trans[0])/(photom_df_length)\n",
    "    print('HERE: wheel 2 photom rate: ', conv_rate)\n",
    "    radians_inds = np.arange(radians.shape[0])\n",
    "    radians_inds[:wheel_trans[0]] = 0\n",
    "    radians_inds[wheel_trans[1]+1:] = 0\n",
    "    rad_subset = radians_inds[wheel_trans[0]:wheel_trans[1]+1]\n",
    "    temp = (rad_subset - wheel_trans[0]) * (1/conv_rate)\n",
    "    radians_inds[wheel_trans[0]:wheel_trans[1]+1] = temp.astype(int)\n",
    "    \n",
    "    return radians_inds\n",
    "\n",
    "# radians_path = datapath + arduino_folder + radians_folder\n",
    "# ex_file = '/radians_RR20231108_C_2024-02-08-125142-0000_flippedDLC_resnet50_wheel_behaviorSep12shuffle1_500000_filtered.csv'\n",
    "def radians_wrapper(file_r, dlc_file, wheel_trans, photom_df_length, path = datapath+arduino_folder+radians_folder):\n",
    "    \"\"\"_summary_\n",
    "        Run via Big Run Part 1 (refer below). Used for loading in raw behavioral data as well as trial typing\n",
    "        and making behavior cubes\n",
    "        \n",
    "        Loads day_dic. NOTE - several day_dic keys are fronted with \"og\". That is because not all trials (waves) \n",
    "        are used due to photometry related bounds and outliers. Thus later we determine the subset of trials\n",
    "        and make the non-og versions (which are the ones actually used)\n",
    "        \n",
    "    Args:\n",
    "        file_m (str): file name\n",
    "        path (str): path to access file\n",
    "        \n",
    "    Returns:\n",
    "        day_dic: dictionary containing most behaviorally relevant data \n",
    "    \"\"\"\n",
    "    day_dic = {}\n",
    "    rd_df = pd.read_csv(path+file_r, index_col = 0)\n",
    "    day_dic['radians_df'] = rd_df\n",
    "    # plt.figure()\n",
    "    # plt.plot(rd_df['radians_interp'])\n",
    "    # plt.title(file_r[:31])\n",
    "    dlc_df, bodyparts = dh.gen_dlc_df(datapath+arduino_folder+dlc_folder+dlc_file)\n",
    "    # print(rd_df.columns)\n",
    "    \n",
    "    # uncomment soon\n",
    "    day_dic['wheel_trans'] = wheel_trans\n",
    "    radians_inds = interpolate_frame_count(day_dic['radians_df']['radians_interp'], wheel_trans, photom_df_length)\n",
    "    combin_dic = {'radians' : rd_df['radians'],'radians_likelihood' : rd_df['radians_likelihood'],\n",
    "                  'radians_interp' : rd_df['radians_interp'], 'frame_count_1' : radians_inds}\n",
    "    combin_df_temp = pd.DataFrame.from_dict(combin_dic)\n",
    "    \n",
    "    combin_df = pd.concat([combin_df_temp, dlc_df], axis=1)\n",
    "    \n",
    "    day_dic['combin_df'] = combin_df\n",
    "    day_dic['og_waves'] = det_waves(day_dic['radians_df']['radians_interp'], wheel_trans, combin_df, photom_df_length, title=file_r[:31])\n",
    "    day_dic['og_wcube_all'] = bd.gen_manip_cube(np.expand_dims(day_dic['radians_df']['radians_interp'], axis=1), day_dic['og_waves'], back_window=BACK_WINDOW, forward_window=FORWARD_WINDOW)\n",
    "    \n",
    "    return day_dic\n",
    "\n",
    "def det_waves(radians, wheel_trans, combin_df, photom_length, title=''):\n",
    "    \"\"\"_summary_\n",
    "    My thresholding function (uses helper functions from wheel_helper) for movement initiation- trial typing\n",
    "    \"\"\"\n",
    "    \n",
    "    lookrange = [0, len(radians)]\n",
    "    spec_dic = {\"thresh\": 0.002, \"filter_param\" : 3, \"order\" : 1, \"clus_max_interval\" : 300, \"clus_min_range\": 20}\n",
    "    trans_points, cbounds = wh.compute_threshed_disp(radians, lookrange, spec_dic, plot=False)\n",
    "    \n",
    "    trial_cbounds = evaluate_cbounds(cbounds, wheel_trans, radians, combin_df, photom_length)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(radians)\n",
    "    plt.title(title + ' now with evaluated cbounds')\n",
    "    for tc in trial_cbounds:\n",
    "        plt.axvline(tc[0], c='r', alpha=0.7)\n",
    "    plt.xlim(lookrange)\n",
    "    plt.axvline(wheel_trans[0], c='0.3')\n",
    "    plt.axvline(wheel_trans[1], c='0.3')\n",
    "    # plt.ylim([0,5])\n",
    "    return trial_cbounds\n",
    "\n",
    "def evaluate_cbounds(cbounds, wheel_trans, radians, combin_df, photom_length):\n",
    "    \"\"\"_summary_\n",
    "    evaluates potential waves on being within the bounds of photom_df\n",
    "    as well as\n",
    "    the amount of displacement from pre and post movement\n",
    "    \"\"\"\n",
    "    \n",
    "    trial_cbounds = []\n",
    "    min_meanval_change_thresh = 0.05\n",
    "    baseline_diffs_thresh = 0.001\n",
    "    for cbound in cbounds:\n",
    "        cb = cbound[0]\n",
    "        cbe = cbound[1]\n",
    "        assert cbe > cb\n",
    "        if combin_df.loc[cb]['frame_count_1'] - p_BACK_WINDOW < 0 or combin_df.loc[cbe]['frame_count_1'] + p_FORWARD_WINDOW > photom_length:\n",
    "            continue\n",
    "        #old bounds if statement\n",
    "        # if cb - BACK_WINDOW < wheel_trans[0] or cbe + FORWARD_WINDOW > wheel_trans[1]:\n",
    "        pre, post = radians[cb-BACK_WINDOW:cb], radians[cbe: cbe+FORWARD_WINDOW]\n",
    "        pre_mean, post_mean = np.mean(pre), np.mean(post)\n",
    "        pre_diffs, post_diffs = np.diff(pre), np.diff(post)\n",
    "        pre_abs_diff_mean, post_abs_diff_mean = np.mean(np.abs(pre_diffs)), np.mean(np.abs(post_diffs))\n",
    "        if abs(post_mean - pre_mean) >= min_meanval_change_thresh:\n",
    "            if pre_abs_diff_mean < baseline_diffs_thresh and post_abs_diff_mean < baseline_diffs_thresh:\n",
    "                trial_cbounds.append(cbound)    \n",
    "                \n",
    "    return trial_cbounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photom Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "Helper functions for loading in the raw photometry data.\n",
    "\"\"\"\n",
    "def find_subfolder_name(mouse_folder, date, datapath=datapath, fluor_folder=fluor_folder):\n",
    "    subfolders = [a for a in os.listdir(datapath+fluor_folder+mouse_folder) if a[0] != '.']\n",
    "    matched_substring = []\n",
    "    for subfold in subfolders:\n",
    "        if subfold.startswith(date):\n",
    "            matched_substring.append(subfold)\n",
    "    assert len(matched_substring) == 1, 'bish pls'\n",
    "    return '/'+matched_substring[0] + '/Fluorescence.csv'\n",
    "\n",
    "def photom_wrapper(mouse_folder, date, datapath=datapath, fluor_folder=fluor_folder, title=''):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Loads in the correct photometry fluorescence csv. \n",
    "    Takes care of subtracting background ROI (Ch5) from CH1,2,3,4\n",
    "    Ch2 is DCN, Ch3 is thalamus, Ch4 is SNr, and Ch5 is the control background ROI.\n",
    "    \n",
    "    Returns:\n",
    "        sig_df: pandas dataframe containing photometry data \n",
    "    \"\"\"\n",
    "    file_f = find_subfolder_name(mouse_folder, date, datapath=datapath, fluor_folder=fluor_folder)\n",
    "    print('file f: ', file_f)\n",
    "    print(datapath, fluor_folder, mouse_folder, file_f)\n",
    "    flur_df = pd.read_csv(datapath + fluor_folder + mouse_folder + file_f, header=1)\n",
    "    print('flur_df columns: ', flur_df.columns)\n",
    "\n",
    "    #Ch2 is DCN, Ch3 is thalamus, Ch4 is SNr, and Ch5 is the control background ROI.\n",
    "    dat = {}\n",
    "    cntrl_410 = flur_df['CH5-410']\n",
    "    cntrl_470 = flur_df['CH5-470']\n",
    "    dat['CH1-470'] = flur_df['CH1-470']\n",
    "    dat['CH1-410'] = flur_df['CH1-410']\n",
    "    # Preprocessing: subtract control noise\n",
    "    for i, reg_name in zip([2,3,4], ['DCN', 'Thal', 'SNr']):\n",
    "        dat[reg_name + '-470'] = flur_df['CH'+str(i)+'-470']-cntrl_470\n",
    "        dat[reg_name + '-410'] = flur_df['CH'+str(i)+'-410']-cntrl_410\n",
    "        \n",
    "    sig_df = pd.DataFrame.from_dict(dat)\n",
    "    sig_df.plot(figsize=(20,10))\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    return sig_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions (low pass, high pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butter_lowpass(photom, thresh, sampling_rate=30):\n",
    "    b,a = butter(4, thresh, btype='low', fs=sampling_rate)\n",
    "    filtered_photom = np.apply_along_axis(lambda x: filtfilt(b, a, x), axis=0, arr=photom)\n",
    "    return filtered_photom\n",
    "\n",
    "def apply_butter_highpass(photom, thresh, sampling_rate=30):\n",
    "    b,a = butter(2, thresh, btype='high', fs=sampling_rate)\n",
    "    filtered_photom = np.apply_along_axis(lambda x: filtfilt(b, a, x, padtype='even'), axis=0, arr=photom)\n",
    "    return filtered_photom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main wrappers - preprocess and load photometry cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(photom, refpoint_framecount, combin_df, phot_coldic, parameter_dic, manip_fps=100, photom_fps=30, plotter_title=''):\n",
    "    \"\"\"_summary_\n",
    "        preprocessing pipeline that is called on a per-trial basis by function - photom_cube_generate - below\n",
    "    \"\"\"\n",
    "    \n",
    "    # STEP -1: Determine photometry frame boundaries\n",
    "    phot_bounds = [refpoint_framecount-p_BACK_WINDOW, refpoint_framecount+p_FORWARD_WINDOW-1]\n",
    "    print(phot_bounds)\n",
    "\n",
    "    # STEP 0: Extracting raw signal (raw signal - control background) \n",
    "    raw_sig = photom.loc[phot_bounds[0]:phot_bounds[1]].to_numpy()\n",
    "    raw_sig_means = np.mean(raw_sig, axis=0)\n",
    "    raw_sig_keys = list(photom.keys())\n",
    "    print('temp photom shape', raw_sig.shape)\n",
    "\n",
    "    # STEP 1: Low Pass Filtering - noise correction\n",
    "    if parameter_dic['lowpass_threshold_2'] == None:\n",
    "        lowpass_photom = apply_butter_lowpass(raw_sig, parameter_dic['lowpass_threshold']) #4th order butterworth lowpass\n",
    "        lowpass_photom_means = np.mean(lowpass_photom, axis=0)\n",
    "        lowpass_photom_keys = list(photom.keys())\n",
    "        print('lowpass photom shape', lowpass_photom.shape)\n",
    "    else: # differentially lowpass 470 and 410 signal\n",
    "        raw_470 = raw_sig[:,[0,2,4,6]]\n",
    "        raw_410 = raw_sig[:,[1,3,5,7]]\n",
    "        arr_470 = apply_butter_lowpass(raw_470, parameter_dic['lowpass_threshold']) #4th order butterworth lowpass\n",
    "        arr_410 = apply_butter_lowpass(raw_410, parameter_dic['lowpass_threshold_2']) #4th order butterworth lowpass\n",
    "        lowpass_photom = np.array([arr_470[:,0],arr_410[:,0],arr_470[:,1],arr_410[:,1],arr_470[:,2],arr_410[:,2],arr_470[:,3],arr_410[:,3]]).T\n",
    "        lowpass_photom_keys = list(photom.keys())\n",
    "        \n",
    "        print('lowpass photom shape', lowpass_photom.shape)\n",
    "\n",
    "    # STEP 2: Motion correction, plot per region\n",
    "    deltf_intermed = {} #Just aligned 410's to the 470s (reg_410adj)\n",
    "    CH470_movcor = {} #(470-410)/410 half number of channels\n",
    "    CH470_410_ratio = {}\n",
    "    CH470_410_uratio = {}\n",
    "    regions =  ['CH1','DCN','Thal','SNr']\n",
    "    for i,reg in enumerate(regions):\n",
    "        chan_470 = lowpass_photom[:,phot_coldic[reg+'-470']]\n",
    "        chan_410 = lowpass_photom[:,phot_coldic[reg+'-410']]\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x=chan_410, y=chan_470) #from scipy.stats\n",
    "        chan_410_fitted = intercept + slope * chan_410\n",
    "        # just shows the adjusted chan 410\n",
    "        deltf_intermed[reg+'-470'] = chan_470#-np.mean(chan_470)\n",
    "        deltf_intermed[reg+'-410adj'] = chan_410_fitted#-np.mean(chan_410_fitted)\n",
    "        \n",
    "        # shows the delta f/f\n",
    "        CH470_movcor[reg+'-470_movcorr'] = (chan_470 - chan_410_fitted)/chan_410_fitted\n",
    "        CH470_410_ratio[reg+'-470_410_ratio'] = chan_470/chan_410_fitted\n",
    "        CH470_410_uratio[reg+'-470_410_uratio'] = chan_470/chan_410\n",
    "\n",
    "    # STEP 2.1: Save CH470 (untouched) and CH410 (linearly aligned to ch470)\n",
    "    deltaf_im_df = pd.DataFrame.from_dict(deltf_intermed)\n",
    "    deltaf_im_keys = list(deltaf_im_df.keys())\n",
    "    deltaf_im_np = deltaf_im_df.to_numpy()\n",
    "\n",
    "    # STEP 2.2: Save movement corrected CH470:  (470-410_a)/410_a\n",
    "    CH470_movcor_df = pd.DataFrame.from_dict(CH470_movcor)\n",
    "    CH470_movcor_keys = list(CH470_movcor_df.keys())\n",
    "    CH470_movcor_np = CH470_movcor_df.to_numpy() #richard comment - call it chan_470_move_cor\n",
    "    \n",
    "    # STEP 3: Normalization to premovement period - zscoring\n",
    "    # uses a premovement window for mean and std\n",
    "    p_start = parameter_dic['norm_window'][0]\n",
    "    p_end = parameter_dic['norm_window'][1]\n",
    "\n",
    "    # STEP 3.1: zscores on CH470_movcor\n",
    "    means = np.mean(CH470_movcor_np[p_start:p_end,:], axis=0) # find means -pre movement window to 0 (where 0 is back_window_p up)\n",
    "    stds = np.std(CH470_movcor_np[p_start:p_end,:], axis=0)\n",
    "    zscores = (CH470_movcor_np-means)/stds #now F/F0\n",
    "    zscores_keys = ['CH1_zscore', 'DCN_zscore', 'Thal_zscore', 'SNr_zscore']\n",
    "\n",
    "\n",
    "    # output_dic contains: numpy arrays \n",
    "    output_dic = {'raw_sig':raw_sig, 'lowpass_photom':lowpass_photom, \n",
    "                  'deltaf_im_np': deltaf_im_np, 'CH470_movcor_np':CH470_movcor_np, 'zscores':zscores}\n",
    "    \n",
    "    # output_dic_keys contains: lists (with the names for channels for respective numpy arrays)\n",
    "    output_dic_keys = {'raw_sig':raw_sig_keys, 'lowpass_photom':lowpass_photom_keys, \n",
    "                       'deltaf_im_np': deltaf_im_keys, 'CH470_movcor_np':CH470_movcor_keys, 'zscores':zscores_keys}    \n",
    "    return output_dic, output_dic_keys\n",
    "\n",
    "def within_oreg(refpoint_framecount, oreg_list):\n",
    "    \"\"\"\n",
    "    Just returns if a photom frame is inside an outlier region (oreg)\n",
    "    - used by photom_cube_generate\n",
    "    \"\"\"\n",
    "    for pair in oreg_list:\n",
    "        # if inside pair's range expanded by forward and backward window\n",
    "        if pair[0] - p_FORWARD_WINDOW <= refpoint_framecount <= pair[1] + p_BACK_WINDOW: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def photom_cube_generate(photom_df, day_dic, oreg_list, \n",
    "                         parameter_dic ={'lowpass_threshold':6, 'norm_window':[p_BACK_WINDOW-p_PRE_MOVE_WINDOW, p_BACK_WINDOW]},\n",
    "                         title='', phot_coldic_override=None):\n",
    "    \"\"\"\n",
    "    Main wrapper for generating a photom cube from photom_df\n",
    "    \"\"\"\n",
    "    \n",
    "    phot_coldic = {key:i for i,key in enumerate(photom_df.keys())}\n",
    "    if phot_coldic_override != None:\n",
    "        print(\"USING OVERRIDE on photcoldic!\")\n",
    "        phot_coldic = phot_coldic_override\n",
    "    print(phot_coldic)\n",
    "    waves = day_dic['og_waves']\n",
    "    mats_dic = {}\n",
    "    rand_mats_dic = {}\n",
    "    \n",
    "    # change marker\n",
    "    trials_used = []\n",
    "    outlier_trials = []\n",
    "    for trial in range(len(waves)):\n",
    "        print('TRIAL: ' + str(trial))\n",
    "        wave = waves[trial]\n",
    "        refpoint_framecount = int(day_dic['combin_df'].loc[wave[0]]['frame_count_1'])\n",
    "        \n",
    "        upper_phot_frame = (photom_df.shape[0])-30*(FORWARD_WINDOW/100)\n",
    "        lower_phot_frame = 30*(BACK_WINDOW/100)\n",
    "        random_refpoint_framecount = np.random.randint(lower_phot_frame,upper_phot_frame)\n",
    "        \n",
    "        \n",
    "        # trials_used excludes trials which are outside of the bounds\n",
    "        # forward frame is out of photom_df\n",
    "        if refpoint_framecount + 30*(FORWARD_WINDOW/100) > photom_df.shape[0]:\n",
    "            print(\"STOPPING trial addition at trial: \" + str(trial))\n",
    "            break\n",
    "        elif refpoint_framecount - 30*(BACK_WINDOW/100) < 0: #back frame is less than 0\n",
    "            print(\"SKIPPING TRIAL: \", trial)\n",
    "            continue  \n",
    "        elif within_oreg(refpoint_framecount, oreg_list):  \n",
    "            print(\"Trial found inside outlier region\")\n",
    "            outlier_trials.append(trial)\n",
    "            continue  # elif trial in outlier_trials:#     continue #old code\n",
    "        else:\n",
    "            trials_used.append(trial)\n",
    "            output_dic, output_dic_keys = preprocess(photom_df, refpoint_framecount, day_dic['combin_df'], phot_coldic, parameter_dic, plotter_title=title+' trial: ' + str(trial)) #used to also output outlier\n",
    "            \n",
    "            rand_output_dic, rand_output_dic_keys = preprocess(photom_df, random_refpoint_framecount, day_dic['combin_df'], phot_coldic, parameter_dic, plotter_title=title+' trial: ' + str(trial)) #used to also output outlier\n",
    "            for key in output_dic.keys():\n",
    "                if key in mats_dic.keys():\n",
    "                    mats_dic[key].append(output_dic[key])\n",
    "                    rand_mats_dic[key].append(rand_output_dic[key])\n",
    "                else:\n",
    "                    mats_dic[key] = [output_dic[key]]\n",
    "                    rand_mats_dic[key] = [rand_output_dic[key]]\n",
    "    cube_dic = {}\n",
    "    rand_cube_dic = {}\n",
    "    for key in mats_dic.keys():\n",
    "        cube_dic[key] = np.dstack(mats_dic[key])    \n",
    "        rand_cube_dic[key] = np.dstack(rand_mats_dic[key])\n",
    "    # return photom_cube, trials_used\n",
    "    return cube_dic, trials_used, outlier_trials, output_dic_keys, rand_cube_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in light on data, also blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in light on database - written like this cuz there are two files\n",
    "\n",
    "wheel_lightonlightoff = '/Wheel_Vid_LightOnOFF_clean_052024.csv'\n",
    "light_df  = pd.read_csv(datapath + arduino_folder + wheel_lightonlightoff)\n",
    "light_dic = {}\n",
    "light_blacklist = []\n",
    "for i in range(light_df.shape[0]):\n",
    "    sess = light_df.loc[i]['Session']\n",
    "    ssplit = sess.split('_')\n",
    "    mouse_key = ssplit[1]\n",
    "    new_mouse_key = rename_dic[mouse_key]\n",
    "    mouse_name = '/' + ssplit[0]+'_'+new_mouse_key\n",
    "    date = ssplit[2]\n",
    "    date = date.replace('-','_')\n",
    "    print(mouse_name, mouse_key, date)\n",
    "\n",
    "    if np.isnan(light_df.loc[i]['First Frame Light On']):\n",
    "        light_blacklist.append(mouse_name + '-'+date)\n",
    "    else:\n",
    "        light_dic[mouse_name + '-'+date] =  [int(light_df.loc[i]['First Frame Light On']), int(light_df.loc[i]['Last Frame Light On'])]\n",
    "\n",
    "wheel_lightonlightoff = '/Wheel_Vid_LightOnOFF_clean.csv'\n",
    "light_df2  = pd.read_csv(datapath + arduino_folder + wheel_lightonlightoff)\n",
    "light_dic2 = {}\n",
    "light_blacklist = []\n",
    "for i in range(light_df2.shape[0]):\n",
    "    sess = light_df2.loc[i]['Session']\n",
    "    ssplit = sess.split('_')\n",
    "    mouse_key = ssplit[1]\n",
    "    mouse_name = '/' + ssplit[0]+'_'+mouse_key\n",
    "    date = ssplit[2]\n",
    "    date = date[0:4]+'_'+date[4:6]+'_'+date[6:]\n",
    "    date = date.replace('-','_')\n",
    "    print(mouse_name, mouse_key, date)\n",
    "\n",
    "    if np.isnan(light_df2.loc[i]['First Frame Light On']):\n",
    "        light_blacklist.append(mouse_name + '-'+date)\n",
    "    else:\n",
    "        light_dic2[mouse_name + '-'+date] =  [int(light_df2.loc[i]['First Frame Light On']), int(light_df2.loc[i]['Last Frame Light On'])]\n",
    "\n",
    "light_dic.update(light_dic2) #combines the two dictionaries\n",
    "\n",
    "# light_blacklist = ['/RR20231109_F-2024_02_06'] # this is the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Processing Starts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_r = []\n",
    "mouse_folders = []\n",
    "for radians_file in os.listdir(datapath+arduino_folder+radians_folder):\n",
    "    if radians_file.startswith('.'):\n",
    "        continue\n",
    "    files_r.append('/'+radians_file)\n",
    "files_r.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "\n",
    "IMPORTANT - if you loaded in the sess_cage directly (unpickling), don't run this cell\n",
    "- this is for generating a gen cage from the bottom up (starting with behavior)\n",
    "- if you want to just do modifications to the photometry pipeline (ie: preprocessing, outliers, etc) start with big run part 2\n",
    "\n",
    "runs through files in manip_folder and loads behavioral data into day_dic for each session.\n",
    "NOTE - loads files into a gen Cage, NOT a sess_cage\n",
    "\n",
    "avoids:\n",
    "- files in light_blacklist\n",
    "\n",
    "\"\"\"\n",
    "cage = Cage()\n",
    "\n",
    "for radians_file in files_r:\n",
    "    dlc_file = '/' + radians_file[9:]\n",
    "    split_file = radians_file.split('_')\n",
    "    mouse_name = '/' + split_file[1]+'_'+split_file[2]\n",
    "    date = split_file[3][:10]\n",
    "    date = date.replace('-','_')\n",
    "    session_name = mouse_name + '-' +date\n",
    "    if session_name in light_blacklist:\n",
    "        print(\"skipping: \" + str(session_name) + \" because no light on light off data\")\n",
    "        continue \n",
    "    wheel_trans = light_dic[session_name]\n",
    "    \n",
    "    mouse_folder =  mouse_name + photom_addon\n",
    "    mouse_folders.append(mouse_folder)\n",
    "    print(date, mouse_name)\n",
    "    \n",
    "    \n",
    "    # Part 1: load in the photom df\n",
    "    photom_df = photom_wrapper(mouse_folder, date, title=mouse_name + ' ' + date)    \n",
    "    \n",
    "    # Part 2: load in the behavior\n",
    "    day_dic = radians_wrapper(radians_file, dlc_file, wheel_trans, photom_df.shape[0], path =  datapath+arduino_folder+radians_folder)\n",
    "    \n",
    "    if mouse_name in cage.name_2_mouse:\n",
    "        cage.name_2_mouse[mouse_name].add_session(date, day_dic, radians_file, dlc_file, photom_df)\n",
    "    else:\n",
    "        new_mouse = Mouse(mouse_name, mouse_folder)\n",
    "        new_mouse.add_session(date, day_dic, radians_file, dlc_file, photom_df)\n",
    "        cage.add_mouse(new_mouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - photom cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration of parameter dics for exploring different preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_windows = [[p_BACK_WINDOW-p_PRE_MOVE_WINDOW, p_BACK_WINDOW], [p_BACK_WINDOW-p_PRE_MOVE_WINDOW-15, p_BACK_WINDOW-15]]\n",
    "labels = ['minus1', 'minus1_alt']\n",
    "print(norm_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowps = [1,2,4,6,12]\n",
    "pdics_list_temp = []\n",
    "for elem in lowps:\n",
    "    for i,nelem in enumerate(norm_windows):\n",
    "        pdics_list_temp.append([elem,i])\n",
    "pdics_list_temp\n",
    "pdic_list = []\n",
    "for pelem in pdics_list_temp:\n",
    "    pdic = {'lowpass_threshold': pelem[0], 'lowpass_threshold_2': None, 'norm_window': norm_windows[pelem[1]], 'name': str(pelem[0]) + '_' +labels[pelem[1]]}\n",
    "    pdic_list.append(pdic)\n",
    "\n",
    "pdic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note - only run either one of the cells below (not both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Run Part 2: FOR SESS CAGE (loaded in from pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sessname in ordered_sessions:\n",
    "    session = sess_cage.sessions[sessname]\n",
    "    parts = sessname.split('-')\n",
    "    oreg_list = [] # NOTE - I have NOT done outlier detection for the wheel photom dataset\n",
    "    pc_override = None\n",
    "    if sessname == 'RR20240320_G-2024_05_07':\n",
    "        templis = ['CH1-470', 'CH1-410', 'Thal-470', 'Thal-410', 'DCN-470', 'DCN-410','SNr-470', 'SNr-410']\n",
    "        pc_override = {key: i for i,key in enumerate(templis)}\n",
    "        print(pc_override)\n",
    "    for parameter_dic in pdic_list:\n",
    "        name = parameter_dic['name']\n",
    "        cube_dic_o, trials_used_o, outliers, cube_dic_keys, rand_cube_dic = photom_cube_generate(session['photom_df'], session['day_dic'], oreg_list, parameter_dic=parameter_dic, phot_coldic_override=pc_override) \n",
    "        sess_cage.sessions[sessname]['cube_dic_lowp_'+name] = cube_dic_o\n",
    "        sess_cage.sessions[sessname]['rand_cube_dic_lowp_'+name] = rand_cube_dic\n",
    "        sess_cage.sessions[sessname]['outlier_trials'] = outliers\n",
    "        sess_cage.sessions[sessname]['photom_trials_used'] = trials_used_o\n",
    "        sess_cage.sessions[sessname]['cube_dic_keys'] = cube_dic_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Run Part 2: for GEN CAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mouse_name in cage.name_2_mouse.keys():\n",
    "    for date in cage.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        print('SESSION: ', mouse_name, date)\n",
    "        session = cage.name_2_mouse[mouse_name].day_2_session[date]\n",
    "        oreg_list = [] # NOTE - I have NOT done outlier detection for the wheel photom dataset\n",
    "        for parameter_dic in pdic_list:\n",
    "            name = parameter_dic['name']\n",
    "            #return cube_dic, trials_used, outlier_trials, output_dic_keys, rand_cube_dic\n",
    "            cube_dic_o, trials_used_o, outliers, cube_dic_keys, rand_cube_dic = photom_cube_generate(session['photom_df'], session['day_dic'], oreg_list, parameter_dic=parameter_dic) \n",
    "            cage.name_2_mouse[mouse_name].day_2_session[date]['cube_dic_lowp_'+name] = cube_dic_o\n",
    "            cage.name_2_mouse[mouse_name].day_2_session[date]['rand_cube_dic_lowp_'+name] = rand_cube_dic\n",
    "            cage.name_2_mouse[mouse_name].day_2_session[date]['outlier_trials'] = outliers\n",
    "            cage.name_2_mouse[mouse_name].day_2_session[date]['photom_trials_used'] = trials_used_o\n",
    "            cage.name_2_mouse[mouse_name].day_2_session[date]['cube_dic_keys'] = cube_dic_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Misc Cube Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "Just makes some adjustments listed below\n",
    "- og_waves, og_wcube_all, og_endpoints -> waves, wcube_all, endpoints (these are selected from the ogs to NOT include outliers)\n",
    "    - this just allows waves, wcube_all (the behavior cube) to match the trial-dimension of the photom cube\n",
    "- note diff from manip part 3 - we'll make wave_dic later cuz no trial typing yet\n",
    "\"\"\"\n",
    "for mouse_name in cage.name_2_mouse.keys():\n",
    "    for date in cage.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        session = cage.name_2_mouse[mouse_name].day_2_session[date]\n",
    "        og_waves = session['day_dic']['og_waves']\n",
    "        og_trial_inds = np.arange(len(og_waves))\n",
    "        og_wcube_all = session['day_dic']['og_wcube_all']\n",
    "        photom_trials_used = session['photom_trials_used']\n",
    "        #code to adjust og_waves and og_wcube_all\n",
    "        waves = [wav for i, wav in enumerate(og_waves) if i in photom_trials_used]\n",
    "        wcube_all = og_wcube_all[:,:,photom_trials_used]\n",
    "        cage.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['waves'] = waves\n",
    "        cage.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wcube_all'] = wcube_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.5 - Trial Definitions (using stride and stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - I just copied code from my photometry_wheel_analysis notebook into a py file and wrote a wrapper pipeline function that \n",
    "# mods sess_cage in place\n",
    "stsh.stride_stance_pipeline(sess_cage, ordered_sessions)\n",
    "stsh.trial_type_pipeline(sess_cage, ordered_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - pickle sessions from Big Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_sav_folder = '/Pickles'\n",
    "\n",
    "def serialize_sessions_from_cage(folder, cage):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Pickle a gen cage \n",
    "    \"\"\"\n",
    "    for mouse_name in cage.name_2_mouse.keys():\n",
    "        for date in cage.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "            print('SESSION: ', mouse_name, date)\n",
    "            session = cage.name_2_mouse[mouse_name].day_2_session[date]\n",
    "            fname = mouse_name+'-'+date+'.pkl'\n",
    "            print(fname)\n",
    "            # break\n",
    "            with open(folder+fname, 'wb') as f:  # open a text file\n",
    "                pickle.dump(session, f) # serialize the list\n",
    "            f.close()\n",
    "            \n",
    "def serialize_sess_cage(folder, cage):\n",
    "    \"\"\"_summary_\n",
    "    (Re)pickle sess_cage\n",
    "    \"\"\"\n",
    "    for sessname in cage.sessions.keys():\n",
    "        session = cage.sessions[sessname]\n",
    "        fname = '/'+sessname+'.pkl'\n",
    "        print(fname)\n",
    "        # break\n",
    "        with open(folder+fname, 'wb') as f:  # open a text file\n",
    "            pickle.dump(session, f) # serialize the list\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only run one of the below saving cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a sessions cage object (use if loaded in sess_cage)\n",
    "pickle_folder = datapath+pkl_sav_folder + '/Wheel_BigRun_Pickle'\n",
    "serialize_sess_cage(pickle_folder, sess_cage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_folder = datapath+pkl_sav_folder+'/Wheel_BigRun_Pickle'\n",
    "serialize_sessions_from_cage(pickle_folder, cage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compress and save pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_compressed_cage(sess_cage, session_list, mode='wheel'):\n",
    "    comp_sess_cage = sessions_cage()\n",
    "    if mode == 'manip':\n",
    "        cube_dic_name = 'cube_dic_lowp_2_minus1'\n",
    "        rew_cube_dic_name = 'rew_cube_dic_lowp_2_minus1'\n",
    "        rand_cube_dic_name = 'rand_cube_dic_lowp_2_minus1'\n",
    "        keys_keep = ['photom_df', cube_dic_name, rew_cube_dic_name, rand_cube_dic_name, 'outlier_trials', 'photom_trials_used', 'cube_dic_keys', 'manip_file']\n",
    "        day_dic_keys_keep = ['metadata', 'col_dic', 'waves', 'wcube_all', 'wave_dic', 'manip_dist', 'endpoints', 'new_endpoints']\n",
    "    elif mode == 'wheel':\n",
    "        cube_dic_name = 'cube_dic_lowp_2_minus1_alt'\n",
    "        rand_cube_dic_name = 'rand_cube_dic_lowp_2_minus1_alt'\n",
    "        keys_keep = ['photom_df', cube_dic_name, rand_cube_dic_name, 'outlier_trials', 'photom_trials_used', 'cube_dic_keys', 'dlc_file', 'rad_file']\n",
    "        day_dic_keys_keep = ['waves','wcube_all','wave_dic','trial_defs','wheel_trans','stride_stance_dic','hand_peaks_troughs','foot_peaks_troughs']\n",
    "    for sessname in session_list:\n",
    "        session = sess_cage.sessions[sessname]\n",
    "        day_dic = session['day_dic']\n",
    "        day_dic_keep = dict((k, day_dic[k]) for k in day_dic_keys_keep)\n",
    "        session_keep = dict((k, session[k]) for k in keys_keep)\n",
    "        session_keep.update({'day_dic':day_dic_keep})\n",
    "        comp_sess_cage.add_sess(sessname, session_keep)\n",
    "    return comp_sess_cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function gen_compressed_cage is located in the \"HELPER - Compress Cage\" section under Important Classes and Helpers\n",
    "# refer there to see which attributes of sessions are kept and not kept\n",
    "\n",
    "compressed_wheel_cage = gen_compressed_cage(sess_cage, ordered_sessions, mode='wheel')\n",
    "compressed_wheel_pkl_folder = datapath + pkl_sav_folder + '/Compressed_Wheel'\n",
    "serialize_sess_cage(compressed_wheel_pkl_folder, compressed_wheel_cage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PASS 2: Big Run Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) adding front half back half trial types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mouse_name in cage.name_2_mouse.keys():\n",
    "    for date in cage.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "        session = cage.name_2_mouse[mouse_name].day_2_session[date]    \n",
    "        session['day_dic']['wave_dic'] = {}\n",
    "        waves = session['day_dic']['waves']\n",
    "        \n",
    "        wlen = len(waves)\n",
    "        wave_inds = np.arange(wlen)\n",
    "        front_half = wave_inds[:int(wlen/2)]\n",
    "        back_half = wave_inds[int(wlen/2):]\n",
    "        cage.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wave_dic']['front_half'] = front_half\n",
    "        cage.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wave_dic']['back_half'] = back_half    \n",
    "        \n",
    "        rand_wave_inds = np.random.permutation(wave_inds)\n",
    "        fh = rand_wave_inds[:int(wlen/2)]\n",
    "        bh = rand_wave_inds[int(wlen/2):]\n",
    "        cage.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wave_dic']['rand_front_half'] = fh\n",
    "        cage.name_2_mouse[mouse_name].day_2_session[date]['day_dic']['wave_dic']['rand_back_half'] = bh  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis Part 1: Plotting EACH Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cubedics(parameter_dic, cube_dic_type, \n",
    "                  ylim_dic = {'raw_sig': [-1.5,1.5], 'lowpass_photom': [-1.5,1.5], 'deltaf_im_np': [-1,1], 'CH470_movcor_np': [-0.005,0.005], 'CH470_410_ratio_np': [0.998,1.002],\n",
    "                              'zscores': [-4,4], 'zscores_ratio': [-4,4], 'f_f0': [0.998,1.002],\n",
    "                             'CH470_410_uratio_np' : [0.998,1.002], 'zscores_uratio': [-4,4], 'f_f0_u': [0.998,1.002]\n",
    "                             },\n",
    "                  care_about = [True,True,True], heatmap=True,\n",
    "                  save_subfolder='', save_genfolder = output_path + '/Preprocessing', save_label='', no_ylim=False) :\n",
    "    \"\"\"\n",
    "    Summary\n",
    "    -----\n",
    "    Plots and (optionally) saves plots of ch1, dcn, thal, snr - averaged across TRIALS\n",
    "    NOTE - if save_subfolder left as '', no saving occurs\n",
    "    NOTE - currently has two breaks in the for loop (put there during debugging), remove these if want to use this and plot ALL sessions\n",
    "    \n",
    "    Parameters\n",
    "    -----\n",
    "    parameter_dic: dictionary of preprocessing params. should use a parameter_dic created in Big Run Part 2\n",
    "    \n",
    "    cube_dic_type: string selected from ['raw_sig', 'lowpass_photom', 'deltaf_im_np', 'deltaf_np', 'zscores']\n",
    "\n",
    "    # select_from: 'raw_sig', 'lowpass_photom' 'deltaf_im_np' 'CH470_movcor_np' 'CH470_410_ratio_np' 'zscores' 'zscores_ratio' 'f_f0'\n",
    "\n",
    "    \"\"\"\n",
    "    lowp_thresh_used = parameter_dic['lowpass_threshold']\n",
    "    addon = '_lowp_'+parameter_dic['name']\n",
    "    norm_window = parameter_dic['norm_window'] \n",
    "    if no_ylim:\n",
    "        ylim=None\n",
    "    else:\n",
    "        ylim = ylim_dic[cube_dic_type]\n",
    "    for mouse_name in cage.name_2_mouse.keys():\n",
    "        for date in cage.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "            print(mouse_name, date)\n",
    "            session = cage.name_2_mouse[mouse_name].day_2_session[date]    \n",
    "            wave_dic = session['day_dic']['wave_dic']\n",
    "            cube_dic = session['cube_dic'+addon]\n",
    "            cube_dic_keys = session['cube_dic_keys'][cube_dic_type]\n",
    "            # print(\"Cube dic keys: \")\n",
    "            # print(cube_dic_keys)\n",
    "            col_dic = {key: i for i,key in enumerate(cube_dic_keys)}\n",
    "            \n",
    "            if len(list(cube_dic.keys())) == 0:\n",
    "                print(\"NO WAVES\")\n",
    "                continue\n",
    "            cube = cube_dic[cube_dic_type]     \n",
    "\n",
    "            print(np.max(cube), np.min(cube))\n",
    "            #all waves\n",
    "            all_title = mouse_name + '_' + date + ' ' + str(cube.shape[2]) + ' trials'\n",
    "            save_title = mouse_name + '-' + date + '-' + cube_dic_type + '-' + save_label + '.jpg'\n",
    "            save_path = save_genfolder + save_subfolder\n",
    "            save_flag = save_subfolder != ''\n",
    "            if care_about[0]:\n",
    "                ph.visualize_cube(cube, col_dic, time_offset = BACK_WINDOW/100, title=all_title, ylim=ylim, norm_window=norm_window, save_flag=save_flag, save_path=save_path, save_title=save_title, heatmap=heatmap)\n",
    "            if care_about[1]:\n",
    "                front_half_trials = wave_dic['front_half']\n",
    "                front_save =  mouse_name + '-' + date + '-' + cube_dic_type + '-' + save_label + '-front' + '.jpg'\n",
    "                if len(front_half_trials) == 0:\n",
    "                    continue\n",
    "                front_cube = cube[:,:,front_half_trials]\n",
    "                front_title = 'FRONT trials: ' + mouse_name + '_' + date + ' ' + str(front_cube.shape[2]) + ' trials'\n",
    "                ph.visualize_cube(front_cube, col_dic, time_offset = BACK_WINDOW/100, title=front_title, ylim=ylim, norm_window=norm_window, save_flag=save_flag, save_path=save_path, save_title=front_save, heatmap=heatmap)\n",
    "            if care_about[2]:\n",
    "                back_half_trials = wave_dic['back_half']\n",
    "                back_save =  mouse_name + '-' + date + '-' + cube_dic_type + '-' + save_label + '-back' + '.jpg'\n",
    "                all_title = mouse_name + '_' + date + ' ' + str(cube.shape[2]) + ' trials'\n",
    "                back_cube = cube[:,:,back_half_trials]\n",
    "                back_title = 'FRONT trials: ' + mouse_name + '_' + date + ' ' + str(back_cube.shape[2]) + ' trials'\n",
    "                ph.visualize_cube(back_cube, col_dic, time_offset = BACK_WINDOW/100, title=back_title, ylim=ylim, norm_window=norm_window, save_flag=save_flag, save_path=save_path, save_title=back_save, heatmap=heatmap)\n",
    "            \n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP1 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_from:raw_sig', 'lowpass_photom' 'deltaf_im_np' 'CH470_movcor_np'  'zscores' \n",
    "cube_dic_type = 'zscores'\n",
    "parameter_dic = None #TEMP - Please fill this in!!! - ie: pdic_list[0]\n",
    "plot_cubedics(parameter_dic, cube_dic_type, save_subfolder='', save_label='', care_about=[True,True,True], no_ylim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP 2: parsing and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parsing ordered_sessions (from sess_cage) to get groups_dic\n",
    "groups_dic: a dictionary mapping time_zone ('early','mid','late') to a list of corresponding session names (to use to access sess_cage)\n",
    "\"\"\"\n",
    "\n",
    "abcdf_ez_erly = ['2024_02_05', '2024_02_06'] #f exception\n",
    "abcdf_ez_late = ['2024_02_08', '2024_02_09']\n",
    "abcdf_hd_erly = ['2024_02_12', '2024_02_13']\n",
    "abcdf_hd_late = ['2024_02_15', '2024_02_16']\n",
    "abcdf_time_list = [abcdf_ez_erly,abcdf_ez_late,abcdf_hd_erly,abcdf_hd_late]\n",
    "\n",
    "ghijk_ez_erly = ['2024_04_29', '2024_04_30']\n",
    "ghijk_ez_late = ['2024_05_02', '2024_05_03']\n",
    "ghijk_hd_erly = ['2024_05_06', '2024_05_07']\n",
    "ghijk_hd_late = ['2024_05_09', '2024_05_10']\n",
    "ghijk_time_list = [ghijk_ez_erly,ghijk_ez_late,ghijk_hd_erly,ghijk_hd_late]\n",
    "\n",
    "exceptions_dic = {'F_ez_erly': ['2024_02_05', '2024_02_07'], 'K_ez_erly': ['2024_04_30', '2024_05_01'], 'K_hd_late': ['2024_05_09', '2024_05_11']}\n",
    "\n",
    "# ordered_sessions = []\n",
    "# for mouse_name in cage.name_2_mouse.keys():\n",
    "#     for date in cage.name_2_mouse[mouse_name].day_2_session.keys():\n",
    "#         ordered_sessions.append(mouse_name[1:] + '-' + date)\n",
    "\n",
    "groups_dic = {'ez_erly': [], 'ez_late': [], 'hd_erly':[], 'hd_late':[]}\n",
    "for i, timezone in enumerate(list(groups_dic.keys())):\n",
    "    for mouse_ID in ['A','B','C','D','F','G','H','I','J','K']:\n",
    "        mouse_name = full_mouse_name(mouse_ID)\n",
    "        if mouse_ID + '_' + timezone in exceptions_dic.keys():\n",
    "            days = exceptions_dic[mouse_ID + '_' + timezone]\n",
    "            sessnames = [mouse_name + '-' + date for date in days]\n",
    "            groups_dic[timezone] += sessnames\n",
    "        else:\n",
    "            if mouse_ID in ['A','B','C','D','F']:\n",
    "                days = abcdf_time_list[i]\n",
    "            else:\n",
    "                days = ghijk_time_list[i]\n",
    "            sessnames = [mouse_name + '-' + date for date in days]\n",
    "            groups_dic[timezone] += sessnames\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Plotters\n",
    "def gen_behav_cube_lis(session_list, parameter_dic, sess_cage, cube_dic_type = 'zscores', trial_type=None, derivative=True):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -----\n",
    "    list of behavior cubes\n",
    "    \"\"\"\n",
    "    cube_list = []\n",
    "    ses_skip = []\n",
    "    cube_dic_keys = None\n",
    "    for sessname in session_list:\n",
    "        # parts = sessname.split('-')\n",
    "        # session = cage2.name_2_mouse['/' + parts[0]].day_2_session[parts[1]] #used before loading in the cube\n",
    "        session = sess_cage.sessions[sessname] #mod: May 30 2024\n",
    "        cube = session['day_dic']['wcube_all']\n",
    "        if derivative:\n",
    "            cube = np.diff(cube, axis=0) #velocity = derivative of manip_dist over time\n",
    "        \n",
    "            \n",
    "        # manip_vel_mean = np.mean(manip_vel, axis=2) #average over trials\n",
    "        if trial_type != None:\n",
    "            wave_inds = session['day_dic']['wave_dic'][trial_type]\n",
    "            if len(wave_inds) == 0:\n",
    "                ses_skip.append(sessname)\n",
    "                print(sessname)\n",
    "            cube = cube[:,:,wave_inds]\n",
    "        cube_list.append(cube)\n",
    "    return cube_list, cube_dic_keys, ses_skip\n",
    "\n",
    "def gen_cube_list(session_list, parameter_dic, sess_cage, cube_dic_type = 'zscores', trial_type=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----\n",
    "    cube_dic_type: string selected from ['raw_sig', 'lowpass_photom', 'deltaf_im_np', 'deltaf_np', 'zscores']\n",
    "    \"\"\"\n",
    "    cube_list = []\n",
    "    addon = '_lowp_'+parameter_dic['name']\n",
    "    front_addon = ''\n",
    "    # if random:\n",
    "    #     front_addon = 'rand_'\n",
    "    ses_skip = []\n",
    "    for sessname in session_list:\n",
    "        parts = sessname.split('-')\n",
    "        # session = cage.name_2_mouse['/' + parts[0]].day_2_session[parts[1]]\n",
    "        session = sess_cage.sessions[sessname]\n",
    "        cube_dic = session[front_addon + 'cube_dic'+addon]\n",
    "        cube = cube_dic[cube_dic_type]\n",
    "        \n",
    "        if trial_type != None:\n",
    "            # print(session['day_dic']['wave_dic'])\n",
    "            wave_inds = session['day_dic']['wave_dic'][trial_type]\n",
    "            \n",
    "            if len(wave_inds) == 0:\n",
    "                ses_skip.append(sessname)\n",
    "                print(sessname)\n",
    "            cube = cube[:,:,wave_inds]\n",
    "        cube_dic_keys = session['cube_dic_keys'][cube_dic_type]\n",
    "        cube_list.append(cube)\n",
    "    return cube_list, cube_dic_keys, ses_skip\n",
    "\n",
    "def multi_cube_plot(session_list, cube_list, cube_params, cube_dic_type, parameter_dic, title_addon='',\n",
    "                    save_subfolder='', save_genfolder = output_path + '/Preprocessing', save_label='',\n",
    "                   ylim_dic = {'raw_sig': [-1.5,1.5], 'lowpass_photom': [-1.5,1.5], 'deltaf_im_np': [-1,1], 'deltaf_np': [-0.005,0.005], 'zscores': [-4,4]},\n",
    "                   behavior_flag = False):\n",
    "    # master_cubelist_params = ['DCN average zscore','Thal average zscore','SNr average zscore']\n",
    "    col_dic = {elem: i for i, elem in enumerate(cube_params)}    \n",
    "    master_cubelist = []\n",
    "    daycube_list = [np.nanmean(daycube, axis=2) for daycube in cube_list] #list of cubes averaged across trials\n",
    "    mouse_ids, grouped_daycubes = sh.only_group_by_mice(session_list, daycube_list) #makes a list of cubes per mouse (sublists in the list grouped_daycubes)\n",
    "    sesscube_list = [np.dstack(daycube_sublist) for daycube_sublist in grouped_daycubes]\n",
    "    mastercube_list = [np.nanmean(sesscube, axis=2) for sesscube in sesscube_list]\n",
    "    mastercube = np.dstack(mastercube_list)\n",
    "    title=cube_dic_type + '-' + str(parameter_dic) + '-' + title_addon\n",
    "\n",
    "    save_title = '/' + save_label + '-' + cube_dic_type + '.jpg'\n",
    "    save_path = save_genfolder + save_subfolder\n",
    "    save_flag = save_subfolder != ''\n",
    "    norm_window=parameter_dic['norm_window']\n",
    "    if not behavior_flag:\n",
    "        ph.visualize_cube(mastercube, col_dic, BACK_WINDOW/100, title=title, norm_window=norm_window,\n",
    "                      save_flag=save_flag, save_path=save_path, save_title=save_title,\n",
    "                      plot_3D=False, xlabel='Time (s)', ylabel='Z-Score', ylim=ylim_dic[cube_dic_type])\n",
    "    else:\n",
    "        new_norm_win = [(val/30)*100 for val in parameter_dic['norm_window']]\n",
    "        ph.visualize_master_behavcube(mastercube, new_norm_win, 100, 5)\n",
    "        # def visualize_master_behavcube(cube, norm_window, frame_rate, time_offset, title= '', save_flag=False, save_path = '', save_title = '',heatmap=True):\n",
    "    return mastercube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example run of gen_cube_list\n",
    "# cube_dic_type = 'zscores'\n",
    "# param_dic_wheel = {'lowpass_threshold': 2, 'lowpass_threshold_2': None, 'norm_window': [105, 135],'name': '2_minus1_alt'}\n",
    "# cube_list, cube_dic_keys, ses_skip = gen_cube_list(ordered_sessions, param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, trial_type='good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP2 - SAVING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving - NO TRIAL TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_dic_type = 'zscores'\n",
    "param_dic_wheel = {'lowpass_threshold': 2, 'lowpass_threshold_2': None, 'norm_window': [114, 135],'name': '2_minus1_alt'}\n",
    "\n",
    "cube_list, cube_dic_keys, ses_skip = gen_cube_list(ordered_sessions, param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type)\n",
    "master_cube_all = multi_cube_plot(ordered_sessions,cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, \n",
    "                                  save_genfolder = output_path + '/Wheel_Photom_figures', save_subfolder='/allses', save_label='alltrials')\n",
    "\n",
    "b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(ordered_sessions, param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type)\n",
    "b_master_cube_all = multi_cube_plot(ordered_sessions,b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, \n",
    "                                    save_genfolder = output_path + '/Wheel_Behav_figures', save_subfolder='/allses', save_label='alltrials_deriv', \n",
    "                                    behavior_flag=True)\n",
    "\n",
    "b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(ordered_sessions, param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, derivative=False)\n",
    "b_master_cube_all_disp = multi_cube_plot(ordered_sessions,b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, \n",
    "                                    save_genfolder = output_path + '/Wheel_Behav_figures', save_subfolder='/allses', save_label='alltrials_disp',\n",
    "                                    behavior_flag=True)\n",
    "\n",
    "subfolder = '/Wheel' + '/allses'\n",
    "ph.save_cube(master_cube_all,  subfolder, 'alltrials', cube_type = cube_dic_type)\n",
    "ph.save_cube(b_master_cube_all,  subfolder, 'alltrials_behav', cube_type = cube_dic_type, behav=True)\n",
    "ph.save_cube(b_master_cube_all_disp,  subfolder, 'alltrials_behav_disps', cube_type = cube_dic_type, behav=True)\n",
    "\n",
    "time_z_cubes = []\n",
    "for time_z in groups_dic.keys():\n",
    "    cube_list, cube_dic_keys, cube_mean_vals = gen_cube_list(groups_dic[time_z], param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type)\n",
    "    cube = multi_cube_plot(groups_dic[time_z],cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, \n",
    "                           save_genfolder = output_path + '/Wheel_Photom_figures', save_subfolder='/'+time_z, save_label='alltrials')\n",
    "    \n",
    "    b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(groups_dic[time_z], param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type)\n",
    "    b_master_cube_all = multi_cube_plot(groups_dic[time_z],b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel,\n",
    "                            save_genfolder = output_path + '/Wheel_Behav_figures', save_subfolder='/'+time_z, save_label='alltrials_deriv',\n",
    "                            behavior_flag=True)\n",
    "    \n",
    "    b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(groups_dic[time_z], param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, derivative=False)\n",
    "    b_master_cube_all_disp = multi_cube_plot(groups_dic[time_z],b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, \n",
    "                            save_genfolder = output_path + '/Wheel_Behav_figures', save_subfolder='/'+time_z, save_label='alltrials_disp',\n",
    "                            behavior_flag=True)\n",
    "    \n",
    "    subfolder = '/Wheel' + '/' + time_z\n",
    "    ph.save_cube(cube,  subfolder, 'alltrials', cube_type = cube_dic_type)\n",
    "    ph.save_cube(b_master_cube_all,  subfolder, 'alltrials_behav', cube_type = cube_dic_type, behav=True)\n",
    "    ph.save_cube(b_master_cube_all_disp,  subfolder, 'alltrials_behav_disps', cube_type = cube_dic_type, behav=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING - YES TRIAL TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "you could commment out the behavior stuff (lines with variables named 'b_...') if you just want to vis/save photom results\n",
    "\"\"\"\n",
    "cube_dic_type = 'zscores'\n",
    "param_dic_wheel = {'lowpass_threshold': 2, 'lowpass_threshold_2': None, 'norm_window': [114, 135],'name': '2_minus1_alt'}\n",
    "\n",
    "for trial_type in ['good','bad']:\n",
    "    # photom cubes\n",
    "    cube_list, cube_dic_keys, ses_skip = gen_cube_list(ordered_sessions, param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, trial_type=trial_type)\n",
    "    master_cube_all = multi_cube_plot(ordered_sessions,cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, \n",
    "                                      save_genfolder = output_path + '/Wheel_Photom_figures', save_subfolder='/allses', save_label=trial_type)\n",
    "\n",
    "    # behavior cubes - derivative of rads\n",
    "    b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(ordered_sessions, param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, trial_type=trial_type)\n",
    "    b_master_cube_all = multi_cube_plot(ordered_sessions,b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, save_subfolder='',\n",
    "                                        behavior_flag=True)\n",
    "\n",
    "    # behavior cubes - not derivative of rads (keep in mind, there's no normalization rn - these just be raw values)\n",
    "    b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(ordered_sessions, param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, trial_type=trial_type, derivative=False)\n",
    "    b_master_cube_all_disp = multi_cube_plot(ordered_sessions,b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, save_subfolder='',\n",
    "                                        behavior_flag=True)\n",
    "\n",
    "    subfolder = '/Wheel' + '/allses'\n",
    "    ph.save_cube(master_cube_all,  subfolder, trial_type, cube_type = cube_dic_type)\n",
    "    ph.save_cube(b_master_cube_all,  subfolder, trial_type + '_behav', cube_type = cube_dic_type, behav=True)\n",
    "    ph.save_cube(b_master_cube_all_disp,  subfolder, trial_type + '_behav_disps', cube_type = cube_dic_type, behav=True)\n",
    "\n",
    "    time_z_cubes = []\n",
    "    # traversing through time zones\n",
    "    for time_z in groups_dic.keys():\n",
    "        cube_list, cube_dic_keys, cube_mean_vals = gen_cube_list(groups_dic[time_z], param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, trial_type=trial_type)\n",
    "        cube = multi_cube_plot(groups_dic[time_z],cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel,\n",
    "                               save_genfolder = output_path + '/Wheel_Photom_figures', save_subfolder='/'+time_z, save_label=trial_type)\n",
    "        \n",
    "        b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(groups_dic[time_z], param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, trial_type=trial_type)\n",
    "        b_master_cube_all = multi_cube_plot(groups_dic[time_z],b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel,save_subfolder='',\n",
    "                                            behavior_flag=True)\n",
    "        \n",
    "        b_cube_list, b_cube_dic_keys, b_ses_skip = gen_behav_cube_lis(groups_dic[time_z], param_dic_wheel, sess_cage, cube_dic_type=cube_dic_type, derivative=False, trial_type=trial_type)\n",
    "        b_master_cube_all_disp = multi_cube_plot(groups_dic[time_z],b_cube_list,cube_dic_keys, cube_dic_type, param_dic_wheel, save_subfolder='',\n",
    "                                        behavior_flag=True)\n",
    "        \n",
    "        subfolder = '/Wheel' + '/' + time_z\n",
    "        ph.save_cube(cube,  subfolder, trial_type, cube_type = cube_dic_type)\n",
    "        ph.save_cube(b_master_cube_all,  subfolder, trial_type+'_behav', cube_type = cube_dic_type, behav=True)\n",
    "        ph.save_cube(b_master_cube_all_disp,  subfolder, trial_type+'_behav_disps', cube_type = cube_dic_type, behav=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE RUN OF CONTROL PLOT - FRONT HALF VS BACK HALF\n",
    "# parameter_dic = pdic_list[0] \n",
    "\n",
    "# cube_dic_type='zscores'\n",
    "# subfold_name = ''\n",
    "\n",
    "# # select_from:raw_sig', 'lowpass_photom' 'deltaf_im_np' 'CH470_movcor_np' 'CH470_410_ratio_np' 'zscores' 'zscores_ratio' 'f_f0', CH470_410_uratio_np, zscores_uratio', 'f_f0_u'\n",
    "# #groups_dic keys: 'ez_erly', 'ez_late', 'hd_erly', 'hd_late'\n",
    "\n",
    "# for time_z in groups_dic.keys():\n",
    "#     print(time_z)\n",
    "#     cube_list,cube_dic_keys,ses_skip = gen_cube_list(groups_dic[time_z], parameter_dic, cube_dic_type=cube_dic_type, random=False)\n",
    "#     multi_cube_plot(groups_dic[time_z],cube_list,cube_dic_keys, cube_dic_type, parameter_dic, title_addon=time_z.upper(), save_subfolder=subfold_name, save_label=time_z + '-front_half-')\n",
    "#     cube_list,cube_dic_keys,ses_skip = gen_cube_list(groups_dic[time_z], parameter_dic, cube_dic_type=cube_dic_type, random=True)\n",
    "#     multi_cube_plot(groups_dic[time_z],cube_list,cube_dic_keys, cube_dic_type, parameter_dic, title_addon=time_z.upper() + '  -RAND-', save_subfolder=subfold_name, save_label=time_z + '-front_half-')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ARCHAIC) Generate radians pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_radians(dlc_df,title=''):\n",
    "    # STEP 1\n",
    "    from circle_fit import taubinSVD\n",
    "    # point_coordinates = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]])\n",
    "\n",
    "    def determine_circle_params(dlc_df):\n",
    "        marker_dat  =[]\n",
    "        for marker in ['singlet', 'doublet', 'triplet', 'quadruplet']:\n",
    "            pc = dlc_df[[marker+'_x', marker+'_y']].to_numpy()\n",
    "            pc_nonan = pc[~np.isnan(pc).any(axis=1)]\n",
    "            marker_dat.append(pc_nonan)\n",
    "            \n",
    "        marker_dat = np.vstack(marker_dat)\n",
    "\n",
    "        xc, yc, r, sigma = taubinSVD(marker_dat)\n",
    "        plt.figure()\n",
    "        plt.plot(marker_dat[:,0], marker_dat[:,1], 'o')\n",
    "        plt.plot(xc, yc, 'o', c='r')\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        plt.title('all markers and estimated circle')\n",
    "        print(\"coords: \", xc, yc)\n",
    "        print(\"radius: \", r)\n",
    "        return xc, yc, r\n",
    "\n",
    "    xc, yc, r = determine_circle_params(dlc_df)\n",
    "\n",
    "    #STEP 2\n",
    "    markers_full = ['singlet_x', 'singlet_y', 'doublet_x', 'doublet_y', 'triplet_x', 'triplet_y', 'quadruplet_x', 'quadruplet_y']\n",
    "    marker_dic = {key: ind for ind, key in enumerate(markers_full)}\n",
    "    mdlc = dlc_df[['singlet_x', 'singlet_y', 'doublet_x', 'doublet_y', 'triplet_x', 'triplet_y', 'quadruplet_x', 'quadruplet_y']].to_numpy()\n",
    "    markers = ['singlet', 'doublet', 'triplet', 'quadruplet']\n",
    "\n",
    "    def marker_helper(dlc_df, row, prev_marker, markers_full = ['singlet_x', 'singlet_y', 'doublet_x', 'doublet_y', 'triplet_x', 'triplet_y', 'quadruplet_x', 'quadruplet_y']):\n",
    "        \n",
    "        #gets you the row's (no repeats) list of markers present\n",
    "        nonan_markers = remove_dup([marker.split('_')[0] for marker in markers_full if not np.isnan(dlc_df.iloc[row][marker])])\n",
    "        # print(nonan_markers)\n",
    "        tempdic = {mark: ind for ind, mark in enumerate(markers)}\n",
    "        # print(\"prev_marker \" , prev_marker)\n",
    "        # print(nonan_markers)\n",
    "        if len(nonan_markers) == 0: #case 0: no markers -> return np.nan\n",
    "            return np.nan\n",
    "        elif len(nonan_markers) == 1: #case 1: only one marker -> return that marker\n",
    "            return tempdic[nonan_markers[0]]\n",
    "        elif not np.isnan(prev_marker): #edge case for the first one    \n",
    "            for i in range(len(nonan_markers)-1,-1,-1):\n",
    "                if markers.index(nonan_markers[i]) == prev_marker:\n",
    "                    return tempdic[nonan_markers[i]]\n",
    "                elif abs(prev_marker - markers.index(nonan_markers[i])) == 1:\n",
    "                    return tempdic[nonan_markers[i]]\n",
    "                elif prev_marker == 0 and nonan_markers[i] == 'quadruplet':\n",
    "                    return tempdic[nonan_markers[i]]\n",
    "                elif prev_marker == 3 and nonan_markers[i] == 'singlet':\n",
    "                    return tempdic[nonan_markers[i]]\n",
    "        return np.nan\n",
    "\n",
    "    #just removes duplicates, and maintains the same order- that's all this is lmao\n",
    "    def remove_dup(seq):\n",
    "        seen = set()\n",
    "        seen_add = seen.add\n",
    "        return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "                    \n",
    "                    \n",
    "    prev_marker = np.nan\n",
    "    markers_func = []\n",
    "    for i in range(dlc_df.shape[0]):\n",
    "        row_oi = dlc_df.iloc[i]\n",
    "        marker_touse = marker_helper(dlc_df, i, prev_marker)\n",
    "        prev_marker = marker_touse\n",
    "        markers_func.append(marker_touse)\n",
    "        \n",
    "    # STEP 3\n",
    "    centers_sub  = (mdlc - np.array([[xc, yc]] * 4).flatten())\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.plot(centers_sub[:,6], centers_sub[:,7], 'o')\n",
    "    ref_vec = np.array([-1,0])\n",
    "    plt.plot(ref_vec, 'o', c='r')\n",
    "    plt.plot(centers_sub[4000,6], centers_sub[4000,7], 'o', c='g')\n",
    "    plt.plot(centers_sub[6000,6], centers_sub[6000,7], 'o', c='r')\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    rads_dic = {}\n",
    "\n",
    "    for i, mark in enumerate(markers):\n",
    "        print(i*2,(i+1)*2)\n",
    "        singlets = np.dot(centers_sub[:,i*2:(i+1)*2], ref_vec)\n",
    "        ref_vec_magnitude = np.linalg.norm(ref_vec)\n",
    "        vectors_magnitude = np.linalg.norm(centers_sub[:,i*2:(i+1)*2], axis=1)\n",
    "        if i == 0:\n",
    "            print('abe: ', singlets)\n",
    "        singlets/=(ref_vec_magnitude * vectors_magnitude)\n",
    "        if i == 0:\n",
    "            print('abe2: ', singlets)\n",
    "        \n",
    "        \n",
    "        singlets = np.arccos(singlets)\n",
    "        \n",
    "        if i == 0:\n",
    "            print('abe3: ', singlets)\n",
    "        \n",
    "        rads_dic[mark] = singlets\n",
    "        if mark == 'quadruplet':\n",
    "            print('pinpoint ' , singlets[6000])\n",
    "            \n",
    "    for m in markers:\n",
    "        plt.plot(rads_dic[m], label=m)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "\n",
    "    rads = []\n",
    "    for i, m in enumerate(markers_func):\n",
    "        if np.isnan(m):\n",
    "            rads.append(m)\n",
    "        else:\n",
    "            rads.append(rads_dic[markers[m]][i])    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(rads)\n",
    "    plt.title(title)\n",
    "\n",
    "    m_prev = np.nan\n",
    "    prev_val = 0\n",
    "    trads = 0 #this is the accumulator\n",
    "    tracker = []\n",
    "    likelihoods = []\n",
    "    cases = []\n",
    "\n",
    "    for i, m_ind in enumerate(markers_func):\n",
    "        if np.isnan(m_ind):\n",
    "            cases.append(0)\n",
    "            # print('nan case: ', i)\n",
    "            tracker.append(np.nan)  \n",
    "            likelihoods.append(0)\n",
    "        else:\n",
    "            # print('smt case: ', i)\n",
    "            likelihoods.append(dlc_df.iloc[int(i)][markers[m_ind]+'_likelihood'])\n",
    "            if m_ind == m_prev or m_prev == np.nan: #if continuity\n",
    "                cases.append(1)\n",
    "                trads += rads[i]-prev_val\n",
    "                tracker.append(trads)\n",
    "                prev_val = rads[i]\n",
    "                m_prev = m_ind\n",
    "                \n",
    "            elif m_ind != m_prev: #if transition point, add 0 to the accumulator (heuristic)\n",
    "                cases.append(2)\n",
    "                trads += 0\n",
    "                prev_val = rads[i]\n",
    "                m_prev = m_ind\n",
    "                tracker.append(trads)\n",
    "\n",
    "    plt.plot(tracker)\n",
    "    plt.title(title)\n",
    "\n",
    "    tracker = np.array(tracker)\n",
    "    non_nan_index = np.where(~np.isnan(tracker))[0][0]\n",
    "    non_nan_value = tracker[non_nan_index]\n",
    "    tracker = tracker - non_nan_value\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(tracker)\n",
    "    plt.title(title)\n",
    "\n",
    "    # OUTPUT\n",
    "    dlc_df['radians'] = tracker\n",
    "    dlc_df['radians_likelihood'] = likelihoods\n",
    "\n",
    "    dh.interpolate_radians(dlc_df, 0.5, non_nan_index, len_thresh=1e5)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(dlc_df['radians_interp'])\n",
    "    plt.title(title)\n",
    "    \n",
    "# Example use case: \n",
    "# dlc_df, bodyparts = dh.gen_dlc_df(datapath+wheel_base+dlc_base_new+'/'+file)\n",
    "# generate_radians(dlc_df, title=mouse_name+ ' ' + date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
